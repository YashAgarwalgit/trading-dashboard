"""
Clean Stock Service Backend - Optimized for Trading Dashboard
Eliminates redundancy, enhances performance, and maintains full compatibility
"""

import yfinance as yf
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import threading
import time
import json
import os
import sqlite3
from contextlib import contextmanager
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor
import requests
import re
import feedparser
import xml.etree.ElementTree as ET
import urllib.parse
from flask import Flask, jsonify, request, send_from_directory, send_file
from flask_cors import CORS
from flask_socketio import SocketIO, emit
from werkzeug.exceptions import BadRequest
import warnings
warnings.filterwarnings('ignore')


from market_metrics_enhanced import compute_enhanced_market_overview
    
class RateLimiter:
    def __init__(self, max_calls=60, window_seconds=60):  # Reduced from 200 to 60
        self.max_calls = max_calls
        self.window_seconds = window_seconds
        self.calls = deque()
        self._lock = threading.Lock()
        self.last_rate_limit_warning = 0
    
    def can_make_call(self):
        now = time.time()
        with self._lock:
            while self.calls and self.calls[0] <= now - self.window_seconds:
                self.calls.popleft()
            
            if len(self.calls) < self.max_calls:
                self.calls.append(now)
                return True
            
            # Show rate limit warning only once per minute
            if now - self.last_rate_limit_warning > 60:
                print(f"‚ö†Ô∏è Rate limit reached: {len(self.calls)}/{self.max_calls} calls in {self.window_seconds}s window")
                self.last_rate_limit_warning = now
            return False

class StockDataService:
    def __init__(self):
        self.price_cache = {}
        self.cache_ttl = 60  # Increased from 30 to 60 seconds
        self.rate_limiter = RateLimiter(max_calls=60, window_seconds=60)  # Reduced rate limit
        self._ticker_resolution_cache = {}
        self.executor = ThreadPoolExecutor(max_workers=4)  # Reduced from 8 to 4
        self._setup_yfinance_session()
        self._request_delays = {}  # Track last request time per ticker
        
    def _setup_yfinance_session(self):
        """Configure yfinance with proper headers"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
    def _is_cache_valid(self, ticker):
        """Check cache validity"""
        if ticker not in self.price_cache:
            return False
        cached_time = self.price_cache[ticker].get('timestamp')
        return cached_time and (time.time() - cached_time) < self.cache_ttl
    
    def _get_cached_data(self, ticker):
        """Retrieve cached data"""
        return self.price_cache[ticker]['data'] if self._is_cache_valid(ticker) else None
    
    def _cache_data(self, ticker, data):
        """Store data in cache"""
        self.price_cache[ticker] = {'data': data, 'timestamp': time.time()}
    
    def _is_likely_indian_stock(self, ticker):
        """Determine if ticker is likely an Indian stock"""
        t = ticker.upper().strip()
        
        # Already has Indian exchange suffix
        if t.endswith('.NS') or t.endswith('.BO'):
            return True
            
        # Common Indian stock patterns
        indian_patterns = [
            # Indian company naming patterns
            r'^[A-Z]+LTD$',
            r'^[A-Z]+BANK$', 
            r'^[A-Z]+FINANCE$',
            r'^[A-Z]+INDUSTRIES$',
            r'^[A-Z]+MOTORS$',
            r'^[A-Z]+STEEL$',
            r'^[A-Z]+POWER$',
            r'^[A-Z]+INFRA$',
            # Common Indian stock symbols
            r'^(RELIANCE|TCS|INFY|HDFCBANK|ICICIBANK|SBIN|BHARTIARTL|ITC|LT|KOTAKBANK)$',
            r'^(ASIANPAINT|NESTLEIND|HINDUNILVR|MARUTI|TITAN|BAJFINANCE|HCLTECH|WIPRO)$',
            r'^(ULTRACEMCO|AXISBANK|SUNPHARMA|TECHM|TATAMOTORS|POWERGRID|NTPC|ONGC)$'
        ]
        
        for pattern in indian_patterns:
            if re.match(pattern, t):
                return True
                
        return False
    
    def _is_likely_us_stock(self, ticker):
        """Determine if ticker is likely a US stock"""
        t = ticker.upper().strip()
        
        # Common US stock patterns
        us_patterns = [
            # Major US companies
            r'^(AAPL|GOOGL|GOOG|MSFT|AMZN|TSLA|META|NVDA|NFLX|CRM)$',
            r'^(BABA|JNJ|JPM|UNH|HD|PG|BAC|MA|DIS|ADBE|CRM|PYPL|INTC|CSCO|PFE|KO)$',
            r'^(XOM|VZ|WMT|CVX|ABT|TMO|COST|AVGO|ACN|MRK|LLY|QCOM|TXN|HON|UPS|LOW)$',
            # US exchange patterns
            r'^[A-Z]{1,5}$',  # Short ticker symbols (1-5 chars) are typically US
        ]
        
        for pattern in us_patterns:
            if re.match(pattern, t):
                return True
                
        # If it's a short ticker and doesn't match Indian patterns, likely US
        if len(t) <= 5 and not self._is_likely_indian_stock(t):
            return True
            
        return False

    def format_ticker_with_exchange(self, ticker):
        """Smart ticker resolution with US/Indian exchange detection"""
        t = ticker.upper().strip()

        # If already formatted, return as-is
        if t.endswith('.NS') or t.endswith('.BO') or '.' in t:
            return t

        # Check cache first
        if t in self._ticker_resolution_cache:
            return self._ticker_resolution_cache[t]

        # Determine most likely candidates based on ticker pattern
        if self._is_likely_us_stock(t):
            # For US stocks, try US first, then Indian exchanges
            candidates = [t, f"{t}.NS", f"{t}.BO"]
        elif self._is_likely_indian_stock(t):
            # For Indian stocks, try Indian exchanges first
            candidates = [f"{t}.NS", f"{t}.BO", t]
        else:
            # Unknown pattern, try all possibilities
            candidates = [t, f"{t}.NS", f"{t}.BO"]

        # Test each candidate
        for cand in candidates:
            if not self.rate_limiter.can_make_call():
                break
            try:
                hist = yf.Ticker(cand).history(period="1d", interval="1d")
                if not hist.empty:
                    self._ticker_resolution_cache[t] = cand
                    return cand
            except:
                continue

        # Fallback to original ticker
        self._ticker_resolution_cache[t] = t
        return t
    
    def get_stock_data(self, ticker, period="12h", interval="1m"):
        """Get current stock data with enhanced caching and rate limiting"""
        # Check cache first - extend cache for frequently requested stocks
        cached_data = self._get_cached_data(ticker)
        if cached_data:
            return cached_data
            
        # Implement per-ticker throttling
        now = time.time()
        last_request_time = self._request_delays.get(ticker, 0)
        min_delay = 2.0  # Minimum 2 seconds between requests for same ticker
        
        if now - last_request_time < min_delay:
            if cached_data:
                print(f"‚è≥ Throttling {ticker}, returning cached data")
                return cached_data
            else:
                print(f"‚è≥ Throttling {ticker}, waiting...")
                time.sleep(min_delay - (now - last_request_time))
        
        if not self.rate_limiter.can_make_call():
            print(f"üö´ Rate limit exceeded for {ticker}")
            if cached_data:
                return cached_data
            return {"error": "Too Many Requests. Rate limited. Try after a while.", "error_type": "RATE_LIMIT"}
            
        try:
            self._request_delays[ticker] = time.time()  # Track request time
            formatted_ticker = self.format_ticker_with_exchange(ticker)
            stock = yf.Ticker(formatted_ticker)
            
            # Use shorter period for faster requests
            hist_data = stock.history(period="1d", interval="1m")  # Reduced from 2d to 1d (50% reduction)
            
            if hist_data.empty:
                error_result = {
                    "error": f"No data found for '{ticker}'",
                    "error_type": "NO_DATA"
                }
                self._cache_data(ticker, error_result)
                return error_result

            current_price = hist_data['Close'].iloc[-1]
            prev_close = hist_data['Close'].iloc[-2] if len(hist_data) > 1 else current_price
            
            price_change = current_price - prev_close
            percent_change = (price_change / prev_close) * 100 if prev_close != 0 else 0
            
            # Try to get info but don't fail if it's not available (saves API calls)
            try:
                info = stock.info or {}
            except:
                info = {}
            
            result = {
                "symbol": ticker,
                "formatted_symbol": formatted_ticker,
                "current_price": float(current_price),
                "previous_close": float(prev_close),
                "change": float(price_change),
                "change_percent": float(percent_change),
                "volume": 0 if pd.isna(hist_data['Volume'].iloc[-1]) else int(hist_data['Volume'].iloc[-1]),
                "market_cap": info.get('marketCap', 'N/A'),
                "pe_ratio": info.get('trailingPE', 'N/A'),
                "day_high": float(hist_data['High'].max()),  # Use max from available data
                "day_low": float(hist_data['Low'].min()),    # Use min from available data
                "last_updated": datetime.now().isoformat(),
                "currency": info.get('currency', 'INR' if formatted_ticker.endswith(('.NS', '.BO')) else 'USD'),
                "market": "NSE" if formatted_ticker.endswith('.NS') else "BSE" if formatted_ticker.endswith('.BO') else "US",
                "status": "success"
            }
            
            self._cache_data(ticker, result)
            return result
            
        except Exception as e:
            error_msg = str(e)
            if "rate" in error_msg.lower() or "many requests" in error_msg.lower():
                print(f"Watchlist error {ticker}: {error_msg}")
                error_result = {"error": f"Too Many Requests. Rate limited. Try after a while.", "error_type": "RATE_LIMIT"}
            else:
                error_result = {"error": f"Failed to fetch '{ticker}': {error_msg}", "error_type": "API_ERROR"}
            
            # Cache errors for shorter time to retry sooner
            self.price_cache[ticker] = {'data': error_result, 'timestamp': time.time() - (self.cache_ttl - 30)}
            return error_result
    
    def get_historical_data(self, ticker, period="15d", interval="1d", aggregate_hours=None):
        """Get historical data with aggregation support"""
        try:
            formatted_ticker = self.format_ticker_with_exchange(ticker)
            stock = yf.Ticker(formatted_ticker)
            
            if aggregate_hours and aggregate_hours > 1:
                base_data = stock.history(period=period, interval='1h')
                if base_data.empty:
                    return {"error": f"No historical data for {ticker}"}
                hist_data = self._aggregate_hourly_candles(base_data, aggregate_hours)
            else:
                hist_data = stock.history(period=period, interval=interval)
            
            if hist_data.empty:
                return {"error": f"No historical data for {ticker}"}
            
            chart_data = []
            for date, row in hist_data.iterrows():
                # Preserve intraday precision; convert space to 'T'
                if interval != '1d':
                    date_str = date.strftime('%Y-%m-%dT%H:%M:%S')
                else:
                    date_str = date.strftime('%Y-%m-%d')
                chart_data.append({
                    "date": date_str,
                    "open": float(row['Open']) if pd.notna(row['Open']) else 0,
                    "high": float(row['High']) if pd.notna(row['High']) else 0,
                    "low": float(row['Low']) if pd.notna(row['Low']) else 0,
                    "close": float(row['Close']) if pd.notna(row['Close']) else 0,
                    "volume": int(row['Volume']) if pd.notna(row['Volume']) and row['Volume'] > 0 else 0
                })
            
            return {"symbol": ticker, "data": chart_data, "interval": interval, "period": period}
            
        except Exception as e:
            return {"error": str(e)}
    
    def _aggregate_hourly_candles(self, data, target_hours):
        """Aggregate hourly candles into synthetic intervals"""
        try:
            if not hasattr(data, 'iterrows') or target_hours <= 1:
                return data
                
            print(f"üîß Aggregating {len(data)} candles into {target_hours}H intervals")
            print(f"üìä Data columns: {list(data.columns)}")
            
            aggregated_data = []
            data_list = []
            
            for date, row in data.iterrows():
                data_list.append({
                    'date': date,
                    'open': row['Open'],
                    'high': row['High'],
                    'low': row['Low'],
                    'close': row['Close'],
                    'volume': row['Volume']
                })
            
            for i in range(0, len(data_list), target_hours):
                bucket = data_list[i:i + target_hours]
                if bucket:
                    aggregated_data.append({
                        'date': bucket[0]['date'],
                        'open': bucket[0]['open'],
                        'high': max(c['high'] for c in bucket),
                        'low': min(c['low'] for c in bucket),
                        'close': bucket[-1]['close'],
                        'volume': sum(c['volume'] for c in bucket if c['volume'] > 0)
                    })
            
            # Convert back to DataFrame with proper structure
            if aggregated_data:
                df = pd.DataFrame(aggregated_data)
                df.set_index('date', inplace=True)
                # Rename columns to match yfinance format
                df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']
                print(f"‚úÖ Aggregated into {len(df)} intervals")
                return df
            
            print("‚ö†Ô∏è  No aggregated data created")
            return pd.DataFrame()
            
        except Exception as e:
            print(f"‚ùå Aggregation error: {e}")
            print(f"üìä Data type: {type(data)}")
            print(f"üìä Data shape: {getattr(data, 'shape', 'No shape')}")
            print(f"üìä Data columns: {getattr(data, 'columns', 'No columns')}")
            # Return original data on error to prevent complete failure
            return data
    
    def cleanup_expired_cache(self):
        """Clean up expired cache entries"""
        try:
            current_time = time.time()
            expired_keys = []
            
            # Clean up price_cache entries
            for key, (data, timestamp) in self.price_cache.items():
                try:
                    # Ensure timestamp is a float/number for comparison
                    if isinstance(timestamp, str):
                        # Convert ISO string to timestamp
                        from datetime import datetime
                        timestamp = datetime.fromisoformat(timestamp.replace('Z', '+00:00')).timestamp()
                    elif hasattr(timestamp, 'timestamp'):
                        # It's a datetime object
                        timestamp = timestamp.timestamp()
                    
                    if current_time - timestamp > self.cache_ttl:
                        expired_keys.append(key)
                except (ValueError, TypeError, AttributeError) as e:
                    # If there's any issue with timestamp parsing, mark for cleanup
                    print(f"‚ö†Ô∏è Invalid price cache timestamp for key {key}: {e}")
                    expired_keys.append(key)
            
            for key in expired_keys:
                del self.price_cache[key]
                
            # Clean up tech_cache entries
            tech_expired_keys = []
            if hasattr(self, 'tech_cache'):
                for key, cached_entry in self.tech_cache.items():
                    try:
                        # Ensure timestamp is datetime object for comparison
                        cache_timestamp = cached_entry['timestamp']
                        if isinstance(cache_timestamp, str):
                            # Convert ISO string to datetime, then to timestamp
                            from datetime import datetime
                            cache_timestamp = datetime.fromisoformat(cache_timestamp.replace('Z', '+00:00')).timestamp()
                        elif hasattr(cache_timestamp, 'timestamp'):
                            # It's a datetime object
                            cache_timestamp = cache_timestamp.timestamp()
                        
                        # Clean up entries older than 15 minutes
                        if current_time - cache_timestamp > 900:  # 15 minutes
                            tech_expired_keys.append(key)
                    except (KeyError, ValueError, TypeError) as e:
                        # If there's any issue with timestamp parsing, mark for cleanup
                        print(f"‚ö†Ô∏è Invalid cache entry format for key {key}: {e}")
                        tech_expired_keys.append(key)
                
                for key in tech_expired_keys:
                    del self.tech_cache[key]
                
                if tech_expired_keys:
                    print(f"‚úÖ Cleaned up {len(tech_expired_keys)} expired tech cache entries")
                
            # Clean up ticker resolution cache - it stores symbols, not timestamps
            # Simple size-based cleanup
            if len(self._ticker_resolution_cache) > 1000:  # Limit cache size
                # Remove half of the oldest entries (simple FIFO cleanup)
                items_to_remove = len(self._ticker_resolution_cache) // 2
                keys_to_remove = list(self._ticker_resolution_cache.keys())[:items_to_remove]
                ticker_removed = 0
                for key in keys_to_remove:
                    del self._ticker_resolution_cache[key]
                    ticker_removed += 1
                print(f"‚úÖ Cleaned up {ticker_removed} ticker resolution cache entries")
            
            if expired_keys:
                print(f"‚úÖ Cleaned up {len(expired_keys)} expired price cache entries")
                
        except Exception as e:
            print(f"Cache cleanup error: {e}")
            import traceback
            traceback.print_exc()

class PortfolioManager:
    def __init__(self):
        self.data_dir = os.path.join(os.path.dirname(__file__), 'data')
        os.makedirs(self.data_dir, exist_ok=True)
        
        self.portfolios = {}
        self.db_path = os.path.join(self.data_dir, 'trading_platform.db')
        self.portfolio_counter = 1
        
        self.init_database()
        self.load_portfolios_from_db()
        
        print(f"üìä PortfolioManager initialized with {len(self.portfolios)} portfolios")
    
    def init_database(self):
        """Initialize SQLite database"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute('PRAGMA foreign_keys = ON;')
                conn.execute('''
                    CREATE TABLE IF NOT EXISTS portfolios (
                        id TEXT PRIMARY KEY,
                        name TEXT NOT NULL,
                        capital REAL NOT NULL,
                        available_cash REAL NOT NULL,
                        description TEXT DEFAULT '',
                        positions TEXT DEFAULT '{}',
                        created_date TEXT NOT NULL,
                        last_updated TEXT NOT NULL
                    )
                ''')
                
                conn.execute('''
                    CREATE TABLE IF NOT EXISTS portfolio_transactions (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        portfolio_id TEXT NOT NULL,
                        symbol TEXT NOT NULL,
                        transaction_type TEXT NOT NULL,
                        quantity INTEGER NOT NULL,
                        price REAL NOT NULL,
                        total_value REAL NOT NULL,
                        timestamp TEXT NOT NULL,
                        FOREIGN KEY (portfolio_id) REFERENCES portfolios (id)
                    )
                ''')
                
                conn.commit()
                print(f"‚úÖ Database initialized: {self.db_path}")
                
        except Exception as e:
            print(f"‚ùå Database initialization failed: {e}")
    
    def load_portfolios_from_db(self):
        """Load portfolios from database"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute('SELECT * FROM portfolios ORDER BY created_date DESC')
                rows = cursor.fetchall()
                
                for row in rows:
                    try:
                        portfolio_data = {
                            'id': row[0],
                            'name': row[1],
                            'capital': float(row[2]),
                            'available_cash': float(row[3]),
                            'description': row[4] if len(row) > 4 else '',
                            'positions': json.loads(row[5]) if len(row) > 5 and row[5] else {},
                            'created_date': row[6] if len(row) > 6 else '',
                            'last_updated': row[7] if len(row) > 7 else ''
                        }
                        
                        self.portfolios[row[0]] = portfolio_data
                        
                    except Exception as e:
                        print(f"‚ö†Ô∏è Skipping corrupted portfolio: {e}")
                        continue
                
                if self.portfolios:
                    max_id = max([int(pid.split('_')[1]) for pid in self.portfolios.keys() 
                                if pid.startswith('portfolio_')], default=0)
                    self.portfolio_counter = max_id + 1
                
                print(f"üìÅ Loaded {len(self.portfolios)} portfolios")
                
        except Exception as e:
            print(f"‚ùå Database load error: {e}")

    def create_portfolio(self, name, capital, description=""):
        """Create new portfolio"""
        try:
            if not name or not isinstance(name, str):
                raise ValueError("Portfolio name required")
            
            capital_float = float(capital)
            if capital_float <= 0:
                raise ValueError("Capital must be positive")
            
            portfolio_id = f"portfolio_{self.portfolio_counter}"
            current_time = datetime.now().isoformat()
            
            portfolio_data = {
                'id': portfolio_id,
                'name': str(name).strip(),
                'capital': capital_float,
                'available_cash': capital_float,
                'description': str(description).strip(),
                'positions': {},
                'created_date': current_time,
                'last_updated': current_time
            }
            
            with sqlite3.connect(self.db_path) as conn:
                conn.execute('''
                    INSERT INTO portfolios (id, name, capital, available_cash, description, positions, created_date, last_updated)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    portfolio_id,
                    portfolio_data['name'],
                    portfolio_data['capital'],
                    portfolio_data['available_cash'],
                    portfolio_data['description'],
                    json.dumps(portfolio_data['positions']),
                    portfolio_data['created_date'],
                    portfolio_data['last_updated']
                ))
                conn.commit()
            
            self.portfolios[portfolio_id] = portfolio_data
            self.portfolio_counter += 1
            
            print(f"‚úÖ Portfolio created: {name}")
            return portfolio_data
            
        except Exception as e:
            print(f"‚ùå Failed to create portfolio: {e}")
            raise Exception(f"Portfolio creation failed: {str(e)}")
    
    def buy_stock(self, portfolio_id, symbol, quantity, price):
        """Execute buy order"""
        try:
            if portfolio_id not in self.portfolios:
                raise Exception(f"Portfolio {portfolio_id} not found")
            
            portfolio = self.portfolios[portfolio_id]
            total_cost = quantity * price
            
            if portfolio['available_cash'] < total_cost:
                raise Exception(f"Insufficient funds. Available: ${portfolio['available_cash']:.2f}, Required: ${total_cost:.2f}")
            
            if symbol in portfolio['positions']:
                existing = portfolio['positions'][symbol]
                total_quantity = existing['quantity'] + quantity
                total_invested = existing['total_cost'] + total_cost
                avg_price = total_invested / total_quantity
                
                portfolio['positions'][symbol] = {
                    'quantity': total_quantity,
                    'avg_price': avg_price,
                    'total_cost': total_invested,
                    'current_price': price,
                    'last_updated': datetime.now().isoformat()
                }
            else:
                portfolio['positions'][symbol] = {
                    'quantity': quantity,
                    'avg_price': price,
                    'total_cost': total_cost,
                    'current_price': price,
                    'last_updated': datetime.now().isoformat()
                }
            
            portfolio['available_cash'] -= total_cost
            portfolio['last_updated'] = datetime.now().isoformat()
            
            with sqlite3.connect(self.db_path) as conn:
                conn.execute('''
                    UPDATE portfolios 
                    SET positions=?, available_cash=?, last_updated=?
                    WHERE id=?
                ''', (
                    json.dumps(portfolio['positions']),
                    portfolio['available_cash'],
                    portfolio['last_updated'],
                    portfolio_id
                ))
                
                conn.execute('''
                    INSERT INTO portfolio_transactions 
                    (portfolio_id, symbol, transaction_type, quantity, price, total_value, timestamp)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    portfolio_id, symbol, 'BUY', quantity, price, total_cost, datetime.now().isoformat()
                ))
                
                conn.commit()
            
            print(f"‚úÖ Bought {quantity} shares of {symbol}")
            return portfolio
            
        except Exception as e:
            print(f"‚ùå Buy order failed: {e}")
            raise Exception(str(e))
    
    def sell_stock(self, portfolio_id, symbol, quantity, price):
        """Execute sell order"""
        try:
            if portfolio_id not in self.portfolios:
                raise Exception(f"Portfolio {portfolio_id} not found")
            
            portfolio = self.portfolios[portfolio_id]
            
            if symbol not in portfolio['positions']:
                raise Exception(f"No position found for {symbol}")
            
            position = portfolio['positions'][symbol]
            
            if position['quantity'] < quantity:
                raise Exception(f"Insufficient shares. Available: {position['quantity']}, Requested: {quantity}")
            
            total_proceeds = quantity * price
            
            if position['quantity'] == quantity:
                del portfolio['positions'][symbol]
            else:
                position['quantity'] -= quantity
                position['total_cost'] = position['avg_price'] * position['quantity']
                position['last_updated'] = datetime.now().isoformat()
            
            portfolio['available_cash'] += total_proceeds
            portfolio['last_updated'] = datetime.now().isoformat()
            
            with sqlite3.connect(self.db_path) as conn:
                conn.execute('''
                    UPDATE portfolios 
                    SET positions=?, available_cash=?, last_updated=?
                    WHERE id=?
                ''', (
                    json.dumps(portfolio['positions']),
                    portfolio['available_cash'],
                    portfolio['last_updated'],
                    portfolio_id
                ))
                
                conn.execute('''
                    INSERT INTO portfolio_transactions 
                    (portfolio_id, symbol, transaction_type, quantity, price, total_value, timestamp)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    portfolio_id, symbol, 'SELL', quantity, price, total_proceeds, datetime.now().isoformat()
                ))
                
                conn.commit()
            
            print(f"‚úÖ Sold {quantity} shares of {symbol}")
            return portfolio
            
        except Exception as e:
            print(f"‚ùå Sell order failed: {e}")
            raise Exception(str(e))

    def delete_portfolio(self, portfolio_id):
        """Delete portfolio"""
        try:
            if portfolio_id not in self.portfolios:
                raise Exception(f"Portfolio {portfolio_id} not found")
            
            with sqlite3.connect(self.db_path) as conn:
                conn.execute('DELETE FROM portfolio_transactions WHERE portfolio_id = ?', (portfolio_id,))
                conn.execute('DELETE FROM portfolios WHERE id = ?', (portfolio_id,))
                conn.commit()
            
            del self.portfolios[portfolio_id]
            print(f"‚úÖ Portfolio {portfolio_id} deleted")
            return True
            
        except Exception as e:
            print(f"‚ùå Portfolio deletion failed: {e}")
            raise Exception(str(e))
    
    def get_all_portfolios(self):
        return list(self.portfolios.values())
    
    def get_portfolio(self, portfolio_id):
        return self.portfolios.get(portfolio_id)
    
    def get_portfolio_transactions(self, portfolio_id):
        """Get transaction history"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute('''
                    SELECT * FROM portfolio_transactions 
                    WHERE portfolio_id=? 
                    ORDER BY timestamp DESC
                ''', (portfolio_id,))
                
                transactions = []
                for row in cursor.fetchall():
                    transactions.append({
                        'id': row[0],
                        'portfolio_id': row[1],
                        'symbol': row[2],
                        'transaction_type': row[3],
                        'quantity': row[4],
                        'price': row[5],
                        'total_value': row[6],
                        'timestamp': row[7]
                    })
                
                return transactions
                
        except Exception as e:
            print(f"‚ùå Failed to get transactions: {e}")
            return []

# Technical Indicators Configuration - MODULAR TIMEFRAME SYSTEM
TECH_INDICES = {
    'NIFTY50': '^NSEI',
    'S&P500': '^GSPC',
    'USDINR': 'USDINR=X',
    'BANKNIFTY': '^NSEBANK'
}

# MODULAR TIMEFRAME ANALYZER - yfinance optimized with trading-specific analysis
class TimeFrameAnalyzer:
    """Advanced timeframe analyzer with period-appropriate data and indicators - ENHANCED VERSION"""
    
    def __init__(self):
        # ENHANCED: More sophisticated timeframe categories for different trading styles
        self.timeframe_configs = {
            'scalping': {
                'periods': ['1M', '2M', '3M', '5M', '15M'],
                'data_window': '1d',
                'base_interval': '1m',
                'indicators': ['rsi_fast', 'ema_ultra_short', 'stoch_fast', 'volume_flow', 'price_action'],
                'purpose': 'Ultra-short term momentum, tick-level precision, and entry/exit timing',
                'holding_time': 'Seconds to minutes',
                'risk_profile': 'Very High',
                'focus': 'Price action, volume spikes, momentum bursts'
            },
            'intraday': {
                'periods': ['30M', '1H', '2H', '4H'],
                'data_window': '5d', 
                'base_interval': '1h',
                'indicators': ['rsi_standard', 'sma_intraday', 'bollinger', 'adx', 'vwap'],
                'purpose': 'Day trading, hourly trend analysis, and session-based strategies',
                'holding_time': 'Minutes to hours (same day)',
                'risk_profile': 'High',
                'focus': 'Hourly patterns, session dynamics, support/resistance levels'
            },
            'swing': {
                'periods': ['1D', '2D', '3D', '5D', '1W'],
                'data_window': '1mo',
                'base_interval': '1d', 
                'indicators': ['sma_swing', 'ema_medium', 'macd_standard', 'ichimoku', 'fibonacci'],
                'purpose': 'Swing trading, multi-day position analysis, and trend confirmation',
                'holding_time': 'Days to weeks',
                'risk_profile': 'Medium-High',
                'focus': 'Daily patterns, weekly trends, earnings cycles'
            },
            'position': {
                'periods': ['2W', '1M', '2M', '3M', '6M'],
                'data_window': '1y',
                'base_interval': '1wk',
                'indicators': ['sma_long', 'ema_trend', 'macd_slow', 'parabolic_sar', 'monthly_rsi'],
                'purpose': 'Long-term trends, investment decisions, and portfolio allocation',
                'holding_time': 'Weeks to months',
                'risk_profile': 'Medium',
                'focus': 'Monthly cycles, quarterly earnings, long-term trends'
            },
            'investment': {
                'periods': ['1Y', '2Y', '3Y', '5Y'],
                'data_window': 'matched', # Use period-specific data windows
                'base_interval': '1wk', # Weekly for better resolution
                'indicators': ['sma_secular', 'yearly_momentum', 'cycle_analysis', 'macro_trends'],
                'purpose': 'Long-term investment, secular trends, and macro-economic cycles',
                'holding_time': 'Months to years',
                'risk_profile': 'Low-Medium',
                'focus': 'Yearly patterns, economic cycles, secular trends'
            }
        }
        
        # ENHANCED: Period-specific indicator parameters with more granular optimization
        self.indicator_params = {
            'scalping': {
                # Ultra-fast parameters for scalping
                'rsi_period': 7, 'rsi_overbought': 80, 'rsi_oversold': 20,
                'sma_fast': 3, 'sma_slow': 8,
                'ema_fast': 2, 'ema_slow': 5,
                'bollinger_period': 8, 'bollinger_std': 1.8,
                'stoch_k': 3, 'stoch_d': 2,
                'volume_period': 5,
                'macd_fast': 5, 'macd_slow': 13, 'macd_signal': 3,
                'adx_period': 7, 'adx_threshold': 30
            },
            'intraday': {
                # Standard intraday parameters
                'rsi_period': 14, 'rsi_overbought': 70, 'rsi_oversold': 30,
                'sma_fast': 9, 'sma_slow': 21,
                'ema_fast': 8, 'ema_slow': 21,
                'bollinger_period': 20, 'bollinger_std': 2.0,
                'stoch_k': 14, 'stoch_d': 3,
                'volume_period': 20,
                'macd_fast': 12, 'macd_slow': 26, 'macd_signal': 9,
                'adx_period': 14, 'adx_threshold': 25
            },
            'swing': {
                # Swing trading optimized parameters
                'rsi_period': 21, 'rsi_overbought': 65, 'rsi_oversold': 35,
                'sma_fast': 20, 'sma_slow': 50,
                'ema_fast': 12, 'ema_slow': 26,
                'bollinger_period': 20, 'bollinger_std': 2.2,
                'stoch_k': 21, 'stoch_d': 5,
                'volume_period': 30,
                'macd_fast': 12, 'macd_slow': 26, 'macd_signal': 9,
                'adx_period': 14, 'adx_threshold': 20,
                'ichimoku_conversion': 9, 'ichimoku_base': 26, 'ichimoku_span': 52
            },
            'position': {
                # Position trading parameters
                'rsi_period': 28, 'rsi_overbought': 60, 'rsi_oversold': 40,
                'sma_fast': 50, 'sma_slow': 200,
                'ema_fast': 50, 'ema_slow': 200,
                'bollinger_period': 50, 'bollinger_std': 2.5,
                'stoch_k': 28, 'stoch_d': 7,
                'volume_period': 50,
                'macd_fast': 19, 'macd_slow': 39, 'macd_signal': 9,
                'adx_period': 21, 'adx_threshold': 18,
                'trend_strength_period': 100
            },
            'investment': {
                # Long-term investment parameters
                'rsi_period': 50, 'rsi_overbought': 55, 'rsi_oversold': 45,
                'sma_fast': 100, 'sma_slow': 300,
                'ema_fast': 100, 'ema_slow': 300,
                'bollinger_period': 100, 'bollinger_std': 3.0,
                'volume_period': 100,
                'macd_fast': 26, 'macd_slow': 52, 'macd_signal': 18,
                'trend_strength_period': 200,
                'cycle_period': 250  # Yearly cycle analysis
            }
        }
        
        # ENHANCED: Market session awareness for better optimization
        self.market_sessions = {
            'asian': {'start': '21:00', 'end': '06:00', 'volatility': 'Low'},
            'london': {'start': '03:00', 'end': '12:00', 'volatility': 'High'},
            'new_york': {'start': '08:00', 'end': '17:00', 'volatility': 'Very High'},
            'overlap_london_ny': {'start': '08:00', 'end': '12:00', 'volatility': 'Extreme'}
        }
    
    def classify_timeframe(self, period_key):
        """Enhanced timeframe classification with more granular categorization"""
        period_upper = period_key.upper()
        
        # Direct mapping first
        for category, config in self.timeframe_configs.items():
            if period_upper in config['periods']:
                return category
        
        # ENHANCED: Intelligent fallback classification based on period characteristics
        if period_upper.endswith('M'):
            minutes = int(period_upper[:-1])
            if minutes <= 15:
                return 'scalping'
            elif minutes <= 240:  # 4 hours
                return 'intraday'
            else:
                return 'swing'
        elif period_upper.endswith('H'):
            hours = int(period_upper[:-1])
            if hours <= 4:
                return 'intraday'
            else:
                return 'swing'
        elif period_upper.endswith('D'):
            days = int(period_upper[:-1])
            if days <= 7:
                return 'swing'
            else:
                return 'position'
        elif period_upper.endswith('W'):
            weeks = int(period_upper[:-1])
            if weeks <= 4:
                return 'swing'
            else:
                return 'position'
        elif period_upper.endswith('Y'):
            return 'investment'
        else:
            return 'swing'  # Default fallback
    
    def get_optimized_params(self, period_key):
        """Enhanced yfinance parameter optimization with more granular timeframe mapping"""
        category = self.classify_timeframe(period_key)
        base_config = self.timeframe_configs[category]
        
        # ENHANCED GRANULAR TIMEFRAME MAPPING with yfinance constraints and optimization (REDUCED BY 50%)
        optimized_params = {
            # ULTRA SHORT-TERM (Scalping) - Maximum granularity with tick-level precision
            '1M': {'period': '12h', 'interval': '1m', 'category': 'scalping', 'data_points': 195, 'optimization_level': 'maximum'},
            '2M': {'period': '1d', 'interval': '1m', 'category': 'scalping', 'data_points': 390, 'aggregate_minutes': 2, 'optimization_level': 'maximum'},
            '3M': {'period': '1d', 'interval': '1m', 'category': 'scalping', 'data_points': 260, 'aggregate_minutes': 3, 'optimization_level': 'maximum'},
            '5M': {'period': '1d', 'interval': '5m', 'category': 'scalping', 'data_points': 144, 'optimization_level': 'high'}, 
            '15M': {'period': '3d', 'interval': '15m', 'category': 'scalping', 'data_points': 224, 'optimization_level': 'high'},
            
            # SHORT-TERM (Intraday) - Hourly precision with session awareness  
            '30M': {'period': '5d', 'interval': '30m', 'category': 'intraday', 'data_points': 240, 'optimization_level': 'high'},
            '1H': {'period': '5d', 'interval': '1h', 'category': 'intraday', 'data_points': 120, 'optimization_level': 'medium'},
            '2H': {'period': '10d', 'interval': '1h', 'category': 'intraday', 'data_points': 120, 'aggregate_hours': 2, 'optimization_level': 'medium'},
            '3H': {'period': '15d', 'interval': '1h', 'category': 'intraday', 'data_points': 120, 'aggregate_hours': 3, 'optimization_level': 'medium'},
            '4H': {'period': '20d', 'interval': '1h', 'category': 'intraday', 'data_points': 120, 'aggregate_hours': 4, 'optimization_level': 'medium'},
            '6H': {'period': '1mo', 'interval': '1h', 'category': 'intraday', 'data_points': 120, 'aggregate_hours': 6, 'optimization_level': 'medium'},
            
            # MEDIUM-TERM (Swing) - Daily precision with pattern recognition
            '1D': {'period': '2mo', 'interval': '1d', 'category': 'swing', 'data_points': 60, 'optimization_level': 'medium'},
            '2D': {'period': '3mo', 'interval': '1d', 'category': 'swing', 'data_points': 45, 'aggregate_days': 2, 'optimization_level': 'medium'},
            '3D': {'period': '4mo', 'interval': '1d', 'category': 'swing', 'data_points': 40, 'aggregate_days': 3, 'optimization_level': 'medium'},
            '5D': {'period': '6mo', 'interval': '1d', 'category': 'swing', 'data_points': 36, 'aggregate_days': 5, 'optimization_level': 'low'},
            '1W': {'period': '1y', 'interval': '1wk', 'category': 'swing', 'data_points': 52, 'optimization_level': 'low'},
            
            # LONG-TERM (Position) - Weekly/Monthly precision with trend analysis
            '2W': {'period': '6mo', 'interval': '1wk', 'category': 'position', 'data_points': 26, 'aggregate_weeks': 2, 'optimization_level': 'low'},
            '1M': {'period': '1y', 'interval': '1wk', 'category': 'position', 'data_points': 52, 'optimization_level': 'low'},
            '2M': {'period': '2y', 'interval': '1mo', 'category': 'position', 'data_points': 24, 'aggregate_months': 2, 'optimization_level': 'minimal'},
            '3M': {'period': '3y', 'interval': '1mo', 'category': 'position', 'data_points': 36, 'optimization_level': 'minimal'},
            '6M': {'period': '6mo', 'interval': '1d', 'category': 'position', 'data_points': 180, 'optimization_level': 'minimal'},
            
            # INVESTMENT (Long-term) - Matched data periods for accurate analysis
            '1Y': {'period': '1y', 'interval': '1wk', 'category': 'investment', 'data_points': 52, 'aggregate_weeks': 1, 'optimization_level': 'high'},
            '2Y': {'period': '2y', 'interval': '1wk', 'category': 'investment', 'data_points': 104, 'aggregate_weeks': 2, 'optimization_level': 'medium'},
            '3Y': {'period': '3y', 'interval': '1mo', 'category': 'investment', 'data_points': 36, 'aggregate_months': 1, 'optimization_level': 'medium'},
            '5Y': {'period': '5y', 'interval': '1mo', 'category': 'investment', 'data_points': 60, 'aggregate_months': 1, 'optimization_level': 'minimal'}
        }
        
        return optimized_params.get(period_key.upper(), {
            'period': base_config['data_window'],
            'interval': base_config['base_interval'],
            'category': category,
            'data_points': 60,
            'optimization_level': 'medium'
        })
    
    def analyze_timeframe(self, period_key):
        """Enhanced timeframe analysis with comprehensive insights and optimization recommendations"""
        period_upper = period_key.upper()
        
        # Get optimized parameters
        params = self.get_optimized_params(period_upper)
        category = params.get('category', self.classify_timeframe(period_upper))
        config = self.timeframe_configs[category]
        
        # ENHANCED: Market context analysis
        market_context = self._get_market_context(category, period_upper)
        
        # ENHANCED: Optimization recommendations
        optimization_notes = self._generate_optimization_notes(params, category)
        
        return {
            'category': category,
            'config': config,
            'purpose': config['purpose'],
            'holding_time': config['holding_time'],
            'risk_profile': config['risk_profile'],
            'focus_areas': config['focus'],
            'yfinance_period': params.get('period', '1y'),
            'yfinance_interval': params.get('interval', '1d'),
            'indicator_params': self.indicator_params[category],
            'data_points': params.get('data_points', 60),
            'optimization_level': params.get('optimization_level', 'medium'),
            'market_context': market_context,
            'optimization_notes': optimization_notes,
            'aggregation_info': self._get_aggregation_info(params),
            'recommended_indicators': config['indicators'],
            'typical_holding': f"{params.get('data_points', 60)} data points optimized for {category}",
            'session_relevance': self._get_session_relevance(category)
        }
    
    def _get_market_context(self, category, period):
        """Provide market context for the timeframe"""
        context_map = {
            'scalping': f"Focuses on {period} intervals for ultra-short term price movements and market microstructure",
            'intraday': f"Analyzes {period} patterns for same-day trading with session-based strategies",
            'swing': f"Captures {period} trends for multi-day positions with technical pattern focus",
            'position': f"Evaluates {period} cycles for longer-term positioning and trend following",
            'investment': f"Assesses {period} secular trends for long-term investment allocation"
        }
        return context_map.get(category, f"General analysis for {period} timeframe")
    
    def _generate_optimization_notes(self, params, category):
        """Generate specific optimization recommendations"""
        level = params.get('optimization_level', 'medium')
        data_points = params.get('data_points', 60)
        
        notes = {
            'maximum': f"Ultra-high frequency optimization with {data_points} data points for maximum precision",
            'high': f"High-frequency optimization providing {data_points} data points for detailed analysis",
            'medium': f"Balanced optimization with {data_points} data points for reliable signal generation",
            'low': f"Long-term optimization using {data_points} data points for trend confirmation",
            'minimal': f"Macro-level optimization with {data_points} data points for secular trend analysis"
        }
        
        return notes.get(level, f"Standard optimization with {data_points} data points")
    
    def _get_aggregation_info(self, params):
        """Provide aggregation information if applicable"""
        aggregation_keys = ['aggregate_minutes', 'aggregate_hours', 'aggregate_days', 'aggregate_weeks', 'aggregate_months', 'aggregate_quarters']
        
        for key in aggregation_keys:
            if key in params:
                time_unit = key.split('_')[1]
                value = params[key]
                return f"Data aggregated over {value} {time_unit} for smoother analysis"
        
        return "No aggregation - raw timeframe data"
    
    def _get_session_relevance(self, category):
        """Determine market session relevance for the timeframe"""
        session_relevance = {
            'scalping': "Highly relevant - monitor Asian, London, NY sessions for volatility",
            'intraday': "Relevant - consider session opens/closes for strategy timing",
            'swing': "Moderately relevant - weekly session patterns may influence entries",
            'position': "Low relevance - focus on longer-term trends over sessions",
            'investment': "Minimal relevance - macro trends transcend session patterns"
        }
        return session_relevance.get(category, "Standard session relevance")
    
    def get_analysis_summary(self):
        """Enhanced summary of all available timeframe configurations with detailed insights"""
        summary = {
            'categories': {},
            'supported_periods': list(self._get_all_supported_periods()),
            'total_categories': len(self.timeframe_configs),
            'optimization_levels': ['minimal', 'low', 'medium', 'high', 'maximum'],
            'market_sessions': self.market_sessions
        }
        
        for category, config in self.timeframe_configs.items():
            # Find periods that belong to this category
            category_periods = []
            for period in self._get_all_supported_periods():
                if self.classify_timeframe(period) == category:
                    category_periods.append(period)
            
            summary['categories'][category] = {
                'purpose': config['purpose'],
                'holding_time': config['holding_time'],
                'risk_profile': config['risk_profile'],
                'focus_areas': config['focus'],
                'supported_periods': category_periods,
                'indicator_optimization': self.indicator_params[category],
                'data_characteristics': {
                    'data_window': config['data_window'],
                    'base_interval': config['base_interval'],
                    'key_indicators': config['indicators']
                },
                'typical_use_cases': self._get_use_cases(category)
            }
        
        return summary
    
    def _get_all_supported_periods(self):
        """Get all supported periods across all categories"""
        all_periods = set()
        for config in self.timeframe_configs.values():
            all_periods.update(config['periods'])
        
        # Add additional commonly used periods
        additional_periods = ['2M', '3M', '30M', '2D', '2W', '2Y', '3Y', '5Y']
        all_periods.update(additional_periods)
        
        return sorted(list(all_periods))
    
    def _get_use_cases(self, category):
        """Provide specific use cases for each category"""
        use_cases = {
            'scalping': ["High-frequency trading", "Market making", "Arbitrage opportunities", "News-based reactions"],
            'intraday': ["Day trading", "Session-based strategies", "Breakout trading", "Range trading"],
            'swing': ["Swing trading", "Event-driven trading", "Technical pattern trading", "Earnings plays"],
            'position': ["Trend following", "Position trading", "Sector rotation", "Momentum strategies"],
            'investment': ["Long-term investing", "Asset allocation", "Dividend growth", "Value investing"]
        }
        return use_cases.get(category, ["General trading strategies"])

# Initialize the enhanced timeframe analyzer
timeframe_analyzer = TimeFrameAnalyzer()

# ENHANCED TECH_PERIODS with modular timeframe analysis
TECH_PERIODS = {
    # Ultra Short-term (Scalping/Minute-level analysis)
    '1M': {'period': '1d', 'interval': '1m', 'name': '1 Minute', 'category': 'scalping'},
    '5M': {'period': '2d', 'interval': '5m', 'name': '5 Minutes', 'category': 'scalping'},
    '15M': {'period': '5d', 'interval': '15m', 'name': '15 Minutes', 'category': 'scalping'},
    
    # Short-term (Intraday analysis)
    '1H': {'period': '5d', 'interval': '1h', 'name': '1 Hour', 'category': 'intraday'},
    '2H': {'period': '10d', 'interval': '1h', 'name': '2 Hours', 'category': 'intraday', 'aggregate_hours': 2},
    '4H': {'period': '20d', 'interval': '1h', 'name': '4 Hours', 'category': 'intraday', 'aggregate_hours': 4},
    
    # Medium-term (Swing trading)  
    '1D': {'period': '60d', 'interval': '1d', 'name': '1 Day', 'category': 'swing'},
    '3D': {'period': '6mo', 'interval': '1d', 'name': '3 Days', 'category': 'swing', 'aggregate_days': 3},
    '5D': {'period': '1y', 'interval': '1d', 'name': '5 Days', 'category': 'swing', 'aggregate_days': 5},
    
    # Long-term (Position/Investment analysis)
    '1W': {'period': '2y', 'interval': '1wk', 'name': '1 Week', 'category': 'position'},
    '1M': {'period': '5y', 'interval': '1mo', 'name': '1 Month', 'category': 'position'},
    '3M': {'period': '10y', 'interval': '3mo', 'name': '3 Months', 'category': 'position'}
}

def safe_float(value, default=0.0):
    """Safely convert pandas/numpy values to float, handling NaN and inf"""
    import math
    try:
        if hasattr(value, 'item'):
            val = value.item()
        else:
            val = float(value)
        
        if math.isnan(val) or math.isinf(val):
            return default
        return val
    except (ValueError, TypeError):
        return default

def json_serialize_safe(obj):
    """Convert numpy/pandas types to JSON-serializable, handling NaN and inf values"""
    import math
    import numpy as np
    
    if hasattr(obj, 'item'):
        val = obj.item()
        # Handle NaN and inf values
        if isinstance(val, float) and (math.isnan(val) or math.isinf(val)):
            return 0.0
        return val
    elif hasattr(obj, 'tolist'):
        # Handle arrays/series with NaN values
        lst = obj.tolist()
        return [0.0 if isinstance(x, float) and (math.isnan(x) or math.isinf(x)) else x for x in lst]
    elif isinstance(obj, dict):
        return {k: json_serialize_safe(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [json_serialize_safe(v) for v in obj]
    elif isinstance(obj, float) and (math.isnan(obj) or math.isinf(obj)):
        # Handle standalone NaN/inf values
        return 0.0
    else:
        return obj

def calculate_technical_indicators(df, timeframe_category='swing', period_key='1D'):
    """Calculate comprehensive technical indicators with timeframe-specific parameters.
    
    Args:
        df: OHLCV DataFrame
        timeframe_category: 'scalping', 'intraday', 'swing', or 'position'  
        period_key: Original period key for context
        
    Returns:
        dict: Latest values AND time-series arrays optimized for timeframe
    """
    import pandas as pd
    import numpy as np
    
    if len(df) < 10:
        return {'error': 'Insufficient data for technical analysis'}
    
    # Get timeframe-specific parameters
    params = timeframe_analyzer.indicator_params.get(timeframe_category, timeframe_analyzer.indicator_params['swing'])
    
    # Initialize results
    indicators = {'latest': {}, 'series': {}, 'timeframe_info': {
        'category': timeframe_category,
        'period': period_key,
        'data_points': len(df),
        'optimization': f"Optimized for {timeframe_analyzer.timeframe_configs[timeframe_category]['purpose']}"
    }}
    
    try:
        # TIMEFRAME-SPECIFIC RSI CALCULATION
        rsi_period = params['rsi_period']
        delta = df['Close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=rsi_period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=rsi_period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        
        indicators['latest'][f'rsi_{rsi_period}'] = safe_float(rsi.iloc[-1], 50.0) if not rsi.empty else 50.0
        indicators['series'][f'rsi_{rsi_period}'] = rsi.fillna(50.0).tolist()
        
        # RSI Signal Analysis (timeframe-appropriate thresholds)
        rsi_current = indicators['latest'][f'rsi_{rsi_period}']
        if rsi_current > params['rsi_overbought']:
            rsi_signal = 'overbought'
        elif rsi_current < params['rsi_oversold']:
            rsi_signal = 'oversold'
        else:
            rsi_signal = 'neutral'
        indicators['latest']['rsi_signal'] = rsi_signal
        
        # TIMEFRAME-SPECIFIC MOVING AVERAGES
        sma_fast_period = params['sma_fast']
        sma_slow_period = params['sma_slow']
        
        # Fast SMA (timeframe-appropriate)
        sma_fast = df['Close'].rolling(window=sma_fast_period).mean()
        indicators['latest'][f'sma_{sma_fast_period}'] = safe_float(sma_fast.iloc[-1], 0.0) if not sma_fast.empty else 0.0
        indicators['series'][f'sma_{sma_fast_period}'] = sma_fast.fillna(method='bfill').tolist()
        
        # Slow SMA (timeframe-appropriate)
        sma_slow = df['Close'].rolling(window=sma_slow_period).mean()
        indicators['latest'][f'sma_{sma_slow_period}'] = safe_float(sma_slow.iloc[-1], 0.0) if not sma_slow.empty else 0.0
        indicators['series'][f'sma_{sma_slow_period}'] = sma_slow.fillna(method='bfill').tolist()
        
        # SMA Trend Analysis
        sma_trend = 'bullish' if indicators['latest'][f'sma_{sma_fast_period}'] > indicators['latest'][f'sma_{sma_slow_period}'] else 'bearish'
        indicators['latest']['sma_trend'] = sma_trend
        
        # TIMEFRAME-SPECIFIC EMA CALCULATION
        ema_fast_period = params['ema_fast']
        ema_slow_period = params['ema_slow']
        
        ema_fast = df['Close'].ewm(span=ema_fast_period).mean()
        indicators['latest'][f'ema_{ema_fast_period}'] = safe_float(ema_fast.iloc[-1], 0.0) if not ema_fast.empty else 0.0
        indicators['series'][f'ema_{ema_fast_period}'] = ema_fast.tolist()
        
        ema_slow = df['Close'].ewm(span=ema_slow_period).mean()
        indicators['latest'][f'ema_{ema_slow_period}'] = safe_float(ema_slow.iloc[-1], 0.0) if not ema_slow.empty else 0.0
        indicators['series'][f'ema_{ema_slow_period}'] = ema_slow.tolist()
        
        # BOLLINGER BANDS (timeframe-specific parameters)
        if 'bollinger_period' in params:
            bb_period = params['bollinger_period']
            bb_std = params['bollinger_std']
            
            rolling_mean = df['Close'].rolling(window=bb_period).mean()
            rolling_std = df['Close'].rolling(window=bb_period).std()
            
            upper_band = rolling_mean + (rolling_std * bb_std)
            lower_band = rolling_mean - (rolling_std * bb_std)
            
            indicators['latest']['bollinger_upper'] = safe_float(upper_band.iloc[-1], 0.0) if not upper_band.empty else 0.0
            indicators['latest']['bollinger_middle'] = safe_float(rolling_mean.iloc[-1], 0.0) if not rolling_mean.empty else 0.0
            indicators['latest']['bollinger_lower'] = safe_float(lower_band.iloc[-1], 0.0) if not lower_band.empty else 0.0
            
            indicators['series']['bollinger_upper'] = upper_band.fillna(method='bfill').tolist()
            indicators['series']['bollinger_middle'] = rolling_mean.fillna(method='bfill').tolist()
            indicators['series']['bollinger_lower'] = lower_band.fillna(method='bfill').tolist()
            
            # Bollinger Band Position Analysis
            current_price = safe_float(df['Close'].iloc[-1], 0.0)
            bb_position = 'above_upper' if current_price > indicators['latest']['bollinger_upper'] else \
                         'below_lower' if current_price < indicators['latest']['bollinger_lower'] else 'within_bands'
            indicators['latest']['bollinger_position'] = bb_position
        
        # MACD CALCULATION (timeframe-specific parameters)
        if 'macd_fast' in params:
            macd_fast = params['macd_fast']
            macd_slow = params['macd_slow']
            macd_signal_period = params['macd_signal']
            
            ema_fast_macd = df['Close'].ewm(span=macd_fast).mean()
            ema_slow_macd = df['Close'].ewm(span=macd_slow).mean()
            
            macd_line = ema_fast_macd - ema_slow_macd
            macd_signal = macd_line.ewm(span=macd_signal_period).mean()
            macd_histogram = macd_line - macd_signal
            
            indicators['latest']['macd_line'] = safe_float(macd_line.iloc[-1], 0.0) if not macd_line.empty else 0.0
            indicators['latest']['macd_signal'] = safe_float(macd_signal.iloc[-1], 0.0) if not macd_signal.empty else 0.0
            indicators['latest']['macd_histogram'] = safe_float(macd_histogram.iloc[-1], 0.0) if not macd_histogram.empty else 0.0
            
            indicators['series']['macd_line'] = macd_line.tolist()
            indicators['series']['macd_signal'] = macd_signal.tolist()
            indicators['series']['macd_histogram'] = macd_histogram.tolist()
            
            # MACD Signal Analysis
            macd_signal_type = 'bullish' if indicators['latest']['macd_line'] > indicators['latest']['macd_signal'] else 'bearish'
            indicators['latest']['macd_signal_type'] = macd_signal_type
        
        # VOLUME ANALYSIS (enhanced for timeframe)
        if 'Volume' in df.columns and not df['Volume'].empty:
            volume_period = params.get('volume_period', 20)
            volume_avg = df['Volume'].rolling(window=volume_period).mean()
            current_volume = safe_float(df['Volume'].iloc[-1], 0.0)
            avg_volume = safe_float(volume_avg.iloc[-1], 1.0) if not volume_avg.empty else 1.0
            
            volume_ratio = current_volume / avg_volume if avg_volume > 0 else 1.0
            
            indicators['latest']['volume_avg'] = avg_volume
            indicators['latest']['volume_current'] = current_volume  
            indicators['latest']['volume_ratio'] = volume_ratio
            indicators['series']['volume'] = df['Volume'].tolist()
            indicators['series']['volume_avg'] = volume_avg.fillna(method='bfill').tolist()
            
            # Volume Signal
            volume_signal = 'high' if volume_ratio > 1.5 else 'low' if volume_ratio < 0.5 else 'normal'
            indicators['latest']['volume_signal'] = volume_signal
        
        # TIMEFRAME-SPECIFIC ADDITIONAL INDICATORS
        
        # For scalping: Add Stochastic Oscillator
        if timeframe_category == 'scalping' and 'stoch_k' in params:
            stoch_k = params['stoch_k']
            stoch_d = params['stoch_d']
            
            low_min = df['Low'].rolling(window=stoch_k).min()
            high_max = df['High'].rolling(window=stoch_k).max()
            
            k_percent = 100 * ((df['Close'] - low_min) / (high_max - low_min))
            d_percent = k_percent.rolling(window=stoch_d).mean()
            
            indicators['latest']['stoch_k'] = safe_float(k_percent.iloc[-1], 50.0) if not k_percent.empty else 50.0
            indicators['latest']['stoch_d'] = safe_float(d_percent.iloc[-1], 50.0) if not d_percent.empty else 50.0
            indicators['series']['stoch_k'] = k_percent.fillna(50.0).tolist()
            indicators['series']['stoch_d'] = d_percent.fillna(50.0).tolist()
        
        # For intraday: Add ADX
        if timeframe_category == 'intraday' and 'adx_period' in params:
            adx_period = params['adx_period']
            
            # Simplified ADX calculation
            high_diff = df['High'].diff()
            low_diff = df['Low'].diff().abs()
            
            plus_dm = np.where((high_diff > low_diff) & (high_diff > 0), high_diff, 0.0)
            minus_dm = np.where((low_diff > high_diff) & (low_diff > 0), low_diff, 0.0)
            
            tr = np.maximum(df['High'] - df['Low'], 
                           np.maximum(abs(df['High'] - df['Close'].shift(1)), 
                                     abs(df['Low'] - df['Close'].shift(1))))
            
            plus_di = 100 * pd.Series(plus_dm).rolling(window=adx_period).mean() / pd.Series(tr).rolling(window=adx_period).mean()
            minus_di = 100 * pd.Series(minus_dm).rolling(window=adx_period).mean() / pd.Series(tr).rolling(window=adx_period).mean()
            
            dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di)
            adx = dx.rolling(window=adx_period).mean()
            
            indicators['latest']['adx'] = safe_float(adx.iloc[-1], 0.0) if not adx.empty else 0.0
            indicators['latest']['plus_di'] = safe_float(plus_di.iloc[-1], 0.0) if not plus_di.empty else 0.0
            indicators['latest']['minus_di'] = safe_float(minus_di.iloc[-1], 0.0) if not minus_di.empty else 0.0
            
            # ADX Trend Strength
            adx_strength = 'strong' if indicators['latest']['adx'] > params['adx_threshold'] else 'weak'
            indicators['latest']['adx_strength'] = adx_strength
        
        # OVERALL SIGNAL SYNTHESIS (timeframe-appropriate)
        signals = []
        if 'rsi_signal' in indicators['latest'] and indicators['latest']['rsi_signal'] != 'neutral':
            signals.append(indicators['latest']['rsi_signal'])
        if 'sma_trend' in indicators['latest']:
            signals.append(indicators['latest']['sma_trend'])
        if 'macd_signal_type' in indicators['latest']:
            signals.append(indicators['latest']['macd_signal_type'])
        
        # Determine overall signal
        bullish_signals = sum(1 for s in signals if s in ['bullish', 'oversold'])
        bearish_signals = sum(1 for s in signals if s in ['bearish', 'overbought'])
        
        if bullish_signals > bearish_signals:
            overall_signal = 'bullish'
        elif bearish_signals > bullish_signals:
            overall_signal = 'bearish'
        else:
            overall_signal = 'neutral'
        
        indicators['latest']['overall_signal'] = overall_signal
        indicators['latest']['signal_strength'] = max(bullish_signals, bearish_signals) / len(signals) if signals else 0.0
        
        return indicators
        
    except Exception as e:
        print(f"‚ùå Technical indicators calculation error: {str(e)}")
        return {
            'error': f'Calculation failed: {str(e)}',
            'timeframe_info': {'category': timeframe_category, 'period': period_key}
        }
    
    indicators = {}
    series = {}
    
    try:
        closes = df['Close']
        volumes = df['Volume'] if 'Volume' in df.columns else pd.Series([0] * len(df))
        
        # Moving Averages with full series for charting
        sma_20 = closes.rolling(20).mean()
        sma_50 = closes.rolling(50).mean()
        ema_20 = closes.ewm(span=20, adjust=False).mean()
        ema_50 = closes.ewm(span=50, adjust=False).mean()
        
        # Latest values
        indicators['sma_20'] = safe_float(sma_20.iloc[-1], 0.0) if len(sma_20.dropna()) > 0 else 0.0
        indicators['sma_50'] = safe_float(sma_50.iloc[-1], 0.0) if len(sma_50.dropna()) > 0 else 0.0
        indicators['ema_20'] = safe_float(ema_20.iloc[-1], 0.0) if len(ema_20.dropna()) > 0 else 0.0
        indicators['ema_50'] = safe_float(ema_50.iloc[-1], 0.0) if len(ema_50.dropna()) > 0 else 0.0
        
        # Series data for chart overlays
        series['sma_20'] = sma_20.fillna(0).tolist()
        series['sma_50'] = sma_50.fillna(0).tolist()
        series['ema_20'] = ema_20.fillna(0).tolist()
        series['ema_50'] = ema_50.fillna(0).tolist()
        
        # RSI with proper Wilder's smoothing
        if len(closes) >= 15:
            delta = closes.diff()
            gain = delta.clip(lower=0)
            loss = -delta.clip(upper=0)
            
            # Wilder's smoothing (EMA with alpha=1/14)
            roll_gain = gain.ewm(alpha=1/14, min_periods=14, adjust=False).mean()
            roll_loss = loss.ewm(alpha=1/14, min_periods=14, adjust=False).mean()
            
            rs = roll_gain / roll_loss
            rsi_series = 100 - (100 / (1 + rs))
            
            indicators['rsi'] = float(rsi_series.iloc[-1]) if not rsi_series.empty else 50.0
            series['rsi'] = rsi_series.fillna(50).tolist()
        else:
            indicators['rsi'] = 50.0
            series['rsi'] = [50.0] * len(closes)
        
        # MACD with validation and series data
        if len(closes) >= 26:
            ema_12 = closes.ewm(span=12, adjust=False).mean()
            ema_26 = closes.ewm(span=26, adjust=False).mean()
            macd_line = ema_12 - ema_26
            macd_signal = macd_line.ewm(span=9, adjust=False).mean()
            macd_histogram = macd_line - macd_signal
            
            indicators['macd'] = {
                'line': float(macd_line.iloc[-1]),
                'signal': float(macd_signal.iloc[-1]),
                'histogram': float(macd_histogram.iloc[-1])
            }
            
            series['macd_line'] = macd_line.fillna(0).tolist()
            series['macd_signal'] = macd_signal.fillna(0).tolist()
            series['macd_histogram'] = macd_histogram.fillna(0).tolist()
        else:
            indicators['macd'] = {'line': 0.0, 'signal': 0.0, 'histogram': 0.0}
            series['macd_line'] = [0.0] * len(closes)
            series['macd_signal'] = [0.0] * len(closes)
            series['macd_histogram'] = [0.0] * len(closes)
        
        # Bollinger Bands with series data
        if len(closes) >= 20:
            bb_middle = closes.rolling(20).mean()
            bb_std = closes.rolling(20).std()
            bb_upper = bb_middle + (bb_std * 2.0)
            bb_lower = bb_middle - (bb_std * 2.0)
            
            indicators['bollinger'] = {
                'upper': float(bb_upper.iloc[-1]),
                'middle': float(bb_middle.iloc[-1]),
                'lower': float(bb_lower.iloc[-1])
            }
            
            series['bollinger_upper'] = bb_upper.fillna(0).tolist()
            series['bollinger_middle'] = bb_middle.fillna(0).tolist()
            series['bollinger_lower'] = bb_lower.fillna(0).tolist()
        else:
            current_price = float(closes.iloc[-1]) if len(closes) > 0 else 0.0
            indicators['bollinger'] = {'upper': current_price, 'middle': current_price, 'lower': current_price}
            series['bollinger_upper'] = [current_price] * len(closes)
            series['bollinger_middle'] = [current_price] * len(closes)
            series['bollinger_lower'] = [current_price] * len(closes)
        
        # Volume Analysis - FIXED: Flatten structure to match frontend expectations
        if len(volumes.dropna()) >= 20:
            volume_avg_series = volumes.rolling(20).mean()
            volume_avg = volume_avg_series.iloc[-1]
            current_volume = volumes.iloc[-1]
            volume_ratio = (current_volume / volume_avg) if volume_avg and volume_avg > 0 else 1.0
            
            # CRITICAL FIX: Flatten volume structure (remove nested 'volume' object)
            indicators['volume_avg'] = float(volume_avg)
            indicators['current_volume'] = float(current_volume)
            indicators['volume_ratio'] = float(volume_ratio)
            
            series['volume'] = volumes.fillna(0).tolist()
            series['volume_avg'] = volume_avg_series.fillna(0).tolist()
        else:
            indicators['volume_avg'] = 0.0
            indicators['current_volume'] = float(volumes.iloc[-1]) if len(volumes) > 0 else 0.0
            indicators['volume_ratio'] = 1.0
            
            series['volume'] = volumes.fillna(0).tolist()
            series['volume_avg'] = [0.0] * len(volumes)
        
        # Add series data to response for frontend charting
        indicators['series'] = series
        
    except Exception as e:
        print(f"Error calculating indicators: {e}")
        # Graceful fallback with flattened volume structure
        indicators = {
            'sma_20': 0.0, 'sma_50': 0.0, 'ema_20': 0.0, 'ema_50': 0.0,
            'rsi': 50.0,
            'macd': {'line': 0.0, 'signal': 0.0, 'histogram': 0.0},
            'bollinger': {'upper': 0.0, 'middle': 0.0, 'lower': 0.0},
            'volume_avg': 0.0, 'current_volume': 0.0, 'volume_ratio': 1.0,
            'series': {}
        }
    
    return indicators

def validate_ticker(ticker):
    """Validate ticker format"""
    if not ticker or not isinstance(ticker, str):
        raise BadRequest("Ticker must be a non-empty string")
    
    if not re.match(r'^[A-Z0-9.-]', ticker.upper()):
        raise BadRequest("Invalid ticker format")
    
    if len(ticker) > 20:
        raise BadRequest("Ticker too long")
    
    return ticker.upper().strip()

# Initialize services
portfolio_manager = PortfolioManager()
stock_service = StockDataService()

# Flask App Configuration
app = Flask(__name__)
app.config['SECRET_KEY'] = os.environ.get('SECRET_KEY', 'dev-secret-key')
app.config['JSON_SORT_KEYS'] = False
app.config['JSONIFY_PRETTYPRINT_REGULAR'] = False
CORS(app, supports_credentials=True, resources={r"/api/*": {"origins": "*"}})
socketio = SocketIO(app, cors_allowed_origins="*", async_mode='threading')

# Routes
@app.route('/')
def serve_index():
    return send_file('../frontend/index.html')

@app.route('/<path:filename>')
def serve_static(filename):
    try:
        return send_from_directory('../frontend', filename)
    except FileNotFoundError:
        return send_from_directory('../frontend', 'index.html')

@app.route('/api/stock/<ticker>')
def get_stock(ticker):
    try:
        validated_ticker = validate_ticker(ticker)
        data = stock_service.get_stock_data(validated_ticker)
        return jsonify(data)
    except BadRequest as e:
        return jsonify({'error': str(e), 'error_type': 'VALIDATION_ERROR'}), 400

@app.route('/api/stock/<ticker>/history')
def get_stock_history(ticker):
    try:
        validated_ticker = validate_ticker(ticker)
        period = request.args.get('period', '15d')
        data = stock_service.get_historical_data(validated_ticker, period=period)
        return jsonify(data)
    except BadRequest as e:
        return jsonify({'error': str(e), 'error_type': 'VALIDATION_ERROR'}), 400

@app.route('/api/stocks/batch', methods=['POST'])
def get_stocks_batch():
    try:
        request_data = request.get_json()
        if not request_data or 'tickers' not in request_data:
            return jsonify({'error': 'Missing tickers in request body'}), 400
        
        tickers = request_data.get('tickers', [])
        if not tickers or len(tickers) > 50:
            return jsonify({'error': 'Invalid ticker count. Max 50 per batch.'}), 400
        
        def fetch_single_stock(ticker):
            try:
                validated_ticker = validate_ticker(ticker)
                return {'ticker': ticker, 'data': stock_service.get_stock_data(validated_ticker), 'status': 'success'}
            except Exception as e:
                return {'ticker': ticker, 'data': None, 'error': str(e), 'status': 'error'}
        
        with ThreadPoolExecutor(max_workers=8) as executor:
            results = list(executor.map(fetch_single_stock, tickers))
        
        return jsonify({
            'results': results,
            'total_requested': len(tickers),
            'successful': len([r for r in results if r['status'] == 'success']),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/market/enhanced', methods=['POST'])
def get_enhanced_market_overview():
    request_data = request.get_json() or {}
    watchlist_symbols = request_data.get('watchlist', [])
    data = compute_enhanced_market_overview(watchlist_symbols)
    return jsonify(data)

@app.route('/api/technical/indicators', methods=['POST'])
def api_technical_indicators():
    """Enhanced technical indicators endpoint with TimeFrameAnalyzer integration"""
    req = request.get_json(force=True, silent=True) or {}
    ticker = (req.get('ticker') or '').strip().upper()
    period_key = (req.get('period') or '1D').upper()
    
    if not ticker:
        return jsonify({'error': 'Ticker required'}), 400
    
    print(f"üîç Technical Indicators Request: ticker={ticker}, period={period_key}")
    
    # Initialize TimeFrameAnalyzer if not already done
    global timeframe_analyzer
    if 'timeframe_analyzer' not in globals():
        timeframe_analyzer = TimeFrameAnalyzer()
    
    # Get timeframe analysis
    period_info = timeframe_analyzer.analyze_timeframe(period_key)
    timeframe_category = period_info['category']
    yfinance_period = period_info['yfinance_period']
    yfinance_interval = period_info['yfinance_interval']
    
    print(f"üìà Period {period_key} ‚Üí Category: {timeframe_category}, yfinance: {yfinance_period}/{yfinance_interval}")
    
    # CRITICAL FIX: Proper symbol resolution for Indian stocks
    if ticker in TECH_INDICES:
        core_symbol = TECH_INDICES[ticker]
    else:
        # Check if it's an Indian stock that needs .NS suffix
        indian_stocks = [
            'RELIANCE', 'TCS', 'HDFCBANK', 'INFY', 'HINDUNILVR', 'ICICIBANK', 
            'SBIN', 'BHARTIARTL', 'ITC', 'KOTAKBANK', 'LT', 'AXISBANK',
            'ASIANPAINT', 'MARUTI', 'SUNPHARMA', 'NESTLEIND', 'BAJFINANCE',
            'BAJAJFINSV', 'WIPRO', 'ULTRACEMCO', 'TATASTEEL', 'TECHM'
        ]
        
        if ticker in indian_stocks:
            core_symbol = f"{ticker}.NS"
        else:
            core_symbol = ticker
    
    # Enhanced cache key with timeframe category
    cache_key = f"tech_indicators_{core_symbol}_{period_key}_{timeframe_category}_{datetime.now().strftime('%Y%m%d_%H%M')}"
    
    # Check cache first
    current_time = datetime.now()
    if hasattr(stock_service, 'tech_cache'):
        if cache_key in stock_service.tech_cache:
            cached_data = stock_service.tech_cache[cache_key]
            if (current_time - cached_data['timestamp']).seconds < 300:  # 5 min cache
                print(f"üìä Returning cached technical indicators for {core_symbol} ({period_key}/{timeframe_category})")
                return jsonify(cached_data['data'])
    else:
        stock_service.tech_cache = {}
    
    try:
        # Use TimeFrameAnalyzer optimized parameters
        print(f"üîç Fetching {core_symbol} data: period={yfinance_period}, interval={yfinance_interval}, category={timeframe_category}")
        
        # Apply rate limiting using the stock_service object
        if not stock_service.rate_limiter.can_make_call():
            # Check for cached data with extended TTL
            for cached_key, cached_entry in stock_service.tech_cache.items():
                if core_symbol in cached_key and period_key in cached_key:
                    if (current_time - cached_entry['timestamp']).seconds < 900:  # 15 min extended cache
                        print(f"‚è≥ Rate limited, using extended cache for {core_symbol}")
                        cached_entry['data']['cache_status'] = 'rate_limited_extended'
                        return jsonify(cached_entry['data'])
            
            return jsonify({'error': 'Rate limited and no cached data available'}), 429
        
        # Fetch data using optimized parameters
        ticker_obj = yf.Ticker(core_symbol)
        data = ticker_obj.history(
            period=yfinance_period,
            interval=yfinance_interval,
            auto_adjust=True,
            prepost=False
        )
        
        if data.empty:
            print(f"‚ùå No data returned for {core_symbol} with {yfinance_period}/{yfinance_interval}")
            return jsonify({'error': f'No data available for {ticker} ({core_symbol})'}), 404
        
        if len(data) < 10:
            return jsonify({'error': 'Insufficient data for technical analysis'}), 400
        
        print(f"‚úÖ Retrieved {len(data)} data points for {core_symbol} {period_key}")
        
        
        # Calculate timeframe-specific technical indicators
        indicators = calculate_technical_indicators(
            data, 
            timeframe_category=timeframe_category,
            period_key=period_key
        )
        
        # Prepare formatted data for frontend
        formatted_data = []
        for index, row in data.tail(100).iterrows():  # Limit to last 100 points
            formatted_item = {
                'timestamp': index.isoformat(),
                'open': float(row['Open']),
                'high': float(row['High']),
                'low': float(row['Low']),
                'close': float(row['Close']),
                'volume': float(row['Volume']) if 'Volume' in row else 0.0
            }
            formatted_data.append(formatted_item)
        
        response_data = {
            'symbol': ticker,
            'resolved_symbol': core_symbol,
            'period': period_key,
            'timeframe_category': timeframe_category,
            'data': json_serialize_safe(formatted_data),
            'indicators': json_serialize_safe(indicators),
            'data_points': len(formatted_data),
            'total_data_points': len(data),
            'yfinance_params': {
                'period': yfinance_period,
                'interval': yfinance_interval
            },
            'optimization_info': f"Optimized for {timeframe_analyzer.timeframe_configs[timeframe_category]['purpose']}",
            'last_updated': datetime.now().isoformat(),
            'cache_status': 'fresh'
        }
        
        # Cache the successful result
        stock_service.tech_cache[cache_key] = {
            'data': response_data,
            'timestamp': current_time
        }
        
        print(f"‚úÖ Technical indicators calculated for {core_symbol} ({period_key}/{timeframe_category}): {len(formatted_data)} points")
        return jsonify(response_data)
        
    except Exception as e:
        print(f"‚ùå Technical analysis failed for {ticker}: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'Technical analysis failed: {str(e)}'}), 500

# Stock Search API Routes
@app.route('/api/search/<query>')
def search_stocks(query):
    """Comprehensive search for US and Indian stocks with sector classification"""
    suggestions = []
    
    # Comprehensive Indian stocks database with sector classification
    indian_stocks = {
        # Banking & Financial Services
        'HDFCBANK': {'name': 'HDFC Bank Ltd', 'sector': 'Banking & Financial Services'},
        'ICICIBANK': {'name': 'ICICI Bank Ltd', 'sector': 'Banking & Financial Services'},
        'SBIN': {'name': 'State Bank of India', 'sector': 'Banking & Financial Services'},
        'KOTAKBANK': {'name': 'Kotak Mahindra Bank', 'sector': 'Banking & Financial Services'},
        'AXISBANK': {'name': 'Axis Bank Ltd', 'sector': 'Banking & Financial Services'},
        'INDUSINDBK': {'name': 'IndusInd Bank Ltd', 'sector': 'Banking & Financial Services'},
        'BANKBARODA': {'name': 'Bank of Baroda', 'sector': 'Banking & Financial Services'},
        'PNB': {'name': 'Punjab National Bank', 'sector': 'Banking & Financial Services'},
        'YESBANK': {'name': 'Yes Bank Ltd', 'sector': 'Banking & Financial Services'},
        'BAJFINANCE': {'name': 'Bajaj Finance Ltd', 'sector': 'Banking & Financial Services'},
        'BAJAJFINSV': {'name': 'Bajaj Finserv Ltd', 'sector': 'Banking & Financial Services'},
        
        # Information Technology
        'TCS': {'name': 'Tata Consultancy Services', 'sector': 'Information Technology'},
        'INFY': {'name': 'Infosys Ltd', 'sector': 'Information Technology'},
        'HCLTECH': {'name': 'HCL Technologies Ltd', 'sector': 'Information Technology'},
        'WIPRO': {'name': 'Wipro Ltd', 'sector': 'Information Technology'},
        'TECHM': {'name': 'Tech Mahindra Ltd', 'sector': 'Information Technology'},
        'LTI': {'name': 'Larsen & Toubro Infotech', 'sector': 'Information Technology'},
        'MINDTREE': {'name': 'Mindtree Ltd', 'sector': 'Information Technology'},
        
        # Oil, Gas & Consumable Fuels
        'RELIANCE': {'name': 'Reliance Industries Ltd', 'sector': 'Oil, Gas & Consumable Fuels'},
        'ONGC': {'name': 'Oil & Natural Gas Corp', 'sector': 'Oil, Gas & Consumable Fuels'},
        'IOC': {'name': 'Indian Oil Corporation Ltd', 'sector': 'Oil, Gas & Consumable Fuels'},
        'BPCL': {'name': 'Bharat Petroleum Corp Ltd', 'sector': 'Oil, Gas & Consumable Fuels'},
        'HPCL': {'name': 'Hindustan Petroleum Corp', 'sector': 'Oil, Gas & Consumable Fuels'},
        'GAIL': {'name': 'GAIL India Ltd', 'sector': 'Oil, Gas & Consumable Fuels'},
        
        # Fast Moving Consumer Goods
        'HINDUNILVR': {'name': 'Hindustan Unilever Ltd', 'sector': 'Fast Moving Consumer Goods'},
        'ITC': {'name': 'ITC Ltd', 'sector': 'Fast Moving Consumer Goods'},
        'NESTLEIND': {'name': 'Nestle India Ltd', 'sector': 'Fast Moving Consumer Goods'},
        'BRITANNIA': {'name': 'Britannia Industries Ltd', 'sector': 'Fast Moving Consumer Goods'},
        'DABUR': {'name': 'Dabur India Ltd', 'sector': 'Fast Moving Consumer Goods'},
        'MARICO': {'name': 'Marico Ltd', 'sector': 'Fast Moving Consumer Goods'},
        'GODREJCP': {'name': 'Godrej Consumer Products', 'sector': 'Fast Moving Consumer Goods'},
        
        # Automobiles
        'MARUTI': {'name': 'Maruti Suzuki India Ltd', 'sector': 'Automobiles'},
        'TATAMOTORS': {'name': 'Tata Motors Ltd', 'sector': 'Automobiles'},
        'M&M': {'name': 'Mahindra & Mahindra Ltd', 'sector': 'Automobiles'},
        'BAJAJ-AUTO': {'name': 'Bajaj Auto Ltd', 'sector': 'Automobiles'},
        'HEROMOTOCO': {'name': 'Hero MotoCorp Ltd', 'sector': 'Automobiles'},
        'TVSMOTORS': {'name': 'TVS Motor Company Ltd', 'sector': 'Automobiles'},
        'EICHERMOT': {'name': 'Eicher Motors Ltd', 'sector': 'Automobiles'},
        
        # Construction & Engineering
        'LT': {'name': 'Larsen & Toubro Ltd', 'sector': 'Construction & Engineering'},
        'ULTRACEMCO': {'name': 'UltraTech Cement Ltd', 'sector': 'Construction & Engineering'},
        'SHREECEM': {'name': 'Shree Cement Ltd', 'sector': 'Construction & Engineering'},
        'ACC': {'name': 'ACC Ltd', 'sector': 'Construction & Engineering'},
        'AMBUJACEMENT': {'name': 'Ambuja Cements Ltd', 'sector': 'Construction & Engineering'},
        
        # Healthcare
        'SUNPHARMA': {'name': 'Sun Pharmaceutical Industries', 'sector': 'Healthcare'},
        'DRREDDY': {'name': 'Dr Reddys Laboratories', 'sector': 'Healthcare'},
        'CIPLA': {'name': 'Cipla Ltd', 'sector': 'Healthcare'},
        'DIVISLAB': {'name': 'Divis Laboratories Ltd', 'sector': 'Healthcare'},
        'BIOCON': {'name': 'Biocon Ltd', 'sector': 'Healthcare'},
        
        # Metals & Mining
        'TATASTEEL': {'name': 'Tata Steel Ltd', 'sector': 'Metals & Mining'},
        'JSWSTEEL': {'name': 'JSW Steel Ltd', 'sector': 'Metals & Mining'},
        'HINDALCO': {'name': 'Hindalco Industries Ltd', 'sector': 'Metals & Mining'},
        'COALINDIA': {'name': 'Coal India Ltd', 'sector': 'Metals & Mining'},
        'SAIL': {'name': 'Steel Authority of India', 'sector': 'Metals & Mining'},
        
        # Telecommunications
        'BHARTIARTL': {'name': 'Bharti Airtel Ltd', 'sector': 'Telecommunications'},
        'RJIO': {'name': 'Reliance Jio Infocomm', 'sector': 'Telecommunications'},
        'IDEA': {'name': 'Vodafone Idea Ltd', 'sector': 'Telecommunications'},
        
        # Power Utilities
        'NTPC': {'name': 'NTPC Ltd', 'sector': 'Power Utilities'},
        'POWERGRID': {'name': 'Power Grid Corporation', 'sector': 'Power Utilities'},
        'ADANIPOWER': {'name': 'Adani Power Ltd', 'sector': 'Power Utilities'},
        'TATAPOWER': {'name': 'Tata Power Co Ltd', 'sector': 'Power Utilities'},
        
        # Capital Goods
        'ABB': {'name': 'ABB India Ltd', 'sector': 'Capital Goods'},
        'BHEL': {'name': 'Bharat Heavy Electricals', 'sector': 'Capital Goods'},
        'CROMPTON': {'name': 'Crompton Greaves Consumer', 'sector': 'Capital Goods'},
        
        # Personal Care
        'TITAN': {'name': 'Titan Company Ltd', 'sector': 'Personal Care'},
        'ASIANPAINT': {'name': 'Asian Paints Ltd', 'sector': 'Personal Care'}
    }
    
    # Major US stocks database with sector classification
    us_stocks = {
        # Technology
        'AAPL': {'name': 'Apple Inc', 'sector': 'Technology'},
        'GOOGL': {'name': 'Alphabet Inc Class A', 'sector': 'Technology'},
        'GOOG': {'name': 'Alphabet Inc Class C', 'sector': 'Technology'},
        'MSFT': {'name': 'Microsoft Corporation', 'sector': 'Technology'},
        'AMZN': {'name': 'Amazon.com Inc', 'sector': 'Technology'},
        'META': {'name': 'Meta Platforms Inc', 'sector': 'Technology'},
        'TSLA': {'name': 'Tesla Inc', 'sector': 'Technology'},
        'NVDA': {'name': 'NVIDIA Corporation', 'sector': 'Technology'},
        'NFLX': {'name': 'Netflix Inc', 'sector': 'Technology'},
        'CRM': {'name': 'Salesforce Inc', 'sector': 'Technology'},
        'ORCL': {'name': 'Oracle Corporation', 'sector': 'Technology'},
        'IBM': {'name': 'International Business Machines', 'sector': 'Technology'},
        'INTC': {'name': 'Intel Corporation', 'sector': 'Technology'},
        'AMD': {'name': 'Advanced Micro Devices', 'sector': 'Technology'},
        'CSCO': {'name': 'Cisco Systems Inc', 'sector': 'Technology'},
        'ADBE': {'name': 'Adobe Inc', 'sector': 'Technology'},
        
        # Healthcare
        'JNJ': {'name': 'Johnson & Johnson', 'sector': 'Healthcare'},
        'PFE': {'name': 'Pfizer Inc', 'sector': 'Healthcare'},
        'UNH': {'name': 'UnitedHealth Group Inc', 'sector': 'Healthcare'},
        'ABBV': {'name': 'AbbVie Inc', 'sector': 'Healthcare'},
        'MRK': {'name': 'Merck & Co Inc', 'sector': 'Healthcare'},
        'TMO': {'name': 'Thermo Fisher Scientific', 'sector': 'Healthcare'},
        'ABT': {'name': 'Abbott Laboratories', 'sector': 'Healthcare'},
        'LLY': {'name': 'Eli Lilly and Company', 'sector': 'Healthcare'},
        'BMY': {'name': 'Bristol-Myers Squibb', 'sector': 'Healthcare'},
        'AMGN': {'name': 'Amgen Inc', 'sector': 'Healthcare'},
        
        # Financial Services
        'BRK.B': {'name': 'Berkshire Hathaway Inc', 'sector': 'Financial Services'},
        'JPM': {'name': 'JPMorgan Chase & Co', 'sector': 'Financial Services'},
        'BAC': {'name': 'Bank of America Corp', 'sector': 'Financial Services'},
        'WFC': {'name': 'Wells Fargo & Company', 'sector': 'Financial Services'},
        'C': {'name': 'Citigroup Inc', 'sector': 'Financial Services'},
        'GS': {'name': 'Goldman Sachs Group Inc', 'sector': 'Financial Services'},
        'MS': {'name': 'Morgan Stanley', 'sector': 'Financial Services'},
        'V': {'name': 'Visa Inc', 'sector': 'Financial Services'},
        'MA': {'name': 'Mastercard Inc', 'sector': 'Financial Services'},
        'PYPL': {'name': 'PayPal Holdings Inc', 'sector': 'Financial Services'},
        
        # Consumer Goods
        'KO': {'name': 'Coca-Cola Company', 'sector': 'Consumer Goods'},
        'PEP': {'name': 'PepsiCo Inc', 'sector': 'Consumer Goods'},
        'PG': {'name': 'Procter & Gamble Co', 'sector': 'Consumer Goods'},
        'WMT': {'name': 'Walmart Inc', 'sector': 'Consumer Goods'},
        'HD': {'name': 'Home Depot Inc', 'sector': 'Consumer Goods'},
        'MCD': {'name': 'McDonalds Corporation', 'sector': 'Consumer Goods'},
        'SBUX': {'name': 'Starbucks Corporation', 'sector': 'Consumer Goods'},
        'NKE': {'name': 'Nike Inc', 'sector': 'Consumer Goods'},
        'COST': {'name': 'Costco Wholesale Corp', 'sector': 'Consumer Goods'},
        
        # Energy
        'XOM': {'name': 'Exxon Mobil Corporation', 'sector': 'Energy'},
        'CVX': {'name': 'Chevron Corporation', 'sector': 'Energy'},
        'COP': {'name': 'ConocoPhillips', 'sector': 'Energy'},
        'EOG': {'name': 'EOG Resources Inc', 'sector': 'Energy'},
        'SLB': {'name': 'Schlumberger NV', 'sector': 'Energy'},
        
        # Communications
        'VZ': {'name': 'Verizon Communications Inc', 'sector': 'Communications'},
        'T': {'name': 'AT&T Inc', 'sector': 'Communications'},
        'TMUS': {'name': 'T-Mobile US Inc', 'sector': 'Communications'},
        'DIS': {'name': 'Walt Disney Company', 'sector': 'Communications'},
        'NFLX': {'name': 'Netflix Inc', 'sector': 'Communications'},
        
        # Industrial
        'BA': {'name': 'Boeing Company', 'sector': 'Industrial'},
        'CAT': {'name': 'Caterpillar Inc', 'sector': 'Industrial'},
        'HON': {'name': 'Honeywell International', 'sector': 'Industrial'},
        'UPS': {'name': 'United Parcel Service', 'sector': 'Industrial'},
        'FDX': {'name': 'FedEx Corporation', 'sector': 'Industrial'},
        'MMM': {'name': '3M Company', 'sector': 'Industrial'},
        'GE': {'name': 'General Electric Company', 'sector': 'Industrial'}
    }
    
    query_upper = query.upper()
    
    # Search Indian stocks first (prioritize Indian market)
    for symbol, data in indian_stocks.items():
        if (query_upper in symbol or 
            query_upper in data['name'].upper() or 
            query_upper in data['sector'].upper()):
            suggestions.append({
                'symbol': symbol,
                'name': data['name'],
                'sector': data['sector'],
                'exchange': 'NSE',
                'formatted_symbol': f"{symbol}.NS"
            })
    
    # Search US stocks
    for symbol, data in us_stocks.items():
        if (query_upper in symbol or 
            query_upper in data['name'].upper() or 
            query_upper in data['sector'].upper()):
            suggestions.append({
                'symbol': symbol,
                'name': data['name'],
                'sector': data['sector'],
                'exchange': 'US',
                'formatted_symbol': symbol
            })
    
    # Return top 20 suggestions with Indian stocks prioritized
    return jsonify({
        'suggestions': suggestions[:20],
        'total_found': len(suggestions),
        'query': query
    })

@app.route('/api/status')
def get_system_status():
    """Get system status for frontend"""
    return jsonify({
        'status': 'connected',
        'active_connections': len(stock_service.price_cache),
        'server_time': datetime.now().isoformat(),
        'data_source': 'Yahoo Finance (yfinance)',
        'supported_markets': ['NSE', 'BSE', 'US'],
        'update_frequency': 'Real-time',
        'cache_entries': len(stock_service.price_cache)
    })

# Portfolio API Routes
@app.route('/api/portfolios', methods=['GET'])
def get_portfolios():
    portfolios = portfolio_manager.get_all_portfolios()
    return jsonify({'portfolios': portfolios, 'count': len(portfolios)})

@app.route('/api/portfolios', methods=['POST'])
def create_portfolio():
    try:
        data = request.get_json()
        name = data.get('name')
        capital = data.get('capital')
        description = data.get('description', '')
        
        portfolio = portfolio_manager.create_portfolio(name, capital, description)
        return jsonify({'success': True, 'portfolio': portfolio})
    except Exception as e:
        return jsonify({'error': str(e)}), 400

@app.route('/api/portfolios/<portfolio_id>', methods=['DELETE'])
def delete_portfolio(portfolio_id):
    try:
        portfolio_manager.delete_portfolio(portfolio_id)
        return jsonify({'success': True})
    except Exception as e:
        return jsonify({'error': str(e)}), 400

@app.route('/api/portfolios/<portfolio_id>/buy', methods=['POST'])
def buy_stock(portfolio_id):
    try:
        data = request.get_json()
        symbol = data.get('symbol')
        quantity = int(data.get('quantity'))
        price = float(data.get('price'))
        
        portfolio = portfolio_manager.buy_stock(portfolio_id, symbol, quantity, price)
        return jsonify({'success': True, 'portfolio': portfolio})
    except Exception as e:
        return jsonify({'error': str(e)}), 400

@app.route('/api/portfolios/<portfolio_id>/sell', methods=['POST'])
def sell_stock(portfolio_id):
    try:
        data = request.get_json()
        symbol = data.get('symbol')
        quantity = int(data.get('quantity'))
        price = float(data.get('price'))
        
        portfolio = portfolio_manager.sell_stock(portfolio_id, symbol, quantity, price)
        return jsonify({'success': True, 'portfolio': portfolio})
    except Exception as e:
        return jsonify({'error': str(e)}), 400

@app.route('/api/portfolios/<portfolio_id>/transactions', methods=['GET'])
def get_portfolio_transactions(portfolio_id):
    transactions = portfolio_manager.get_portfolio_transactions(portfolio_id)
    return jsonify({'transactions': transactions})

@app.route('/api/portfolios/<portfolio_id>/value', methods=['GET'])
def get_portfolio_value(portfolio_id):
    """Get real-time portfolio value with current prices"""
    try:
        portfolio = portfolio_manager.get_portfolio(portfolio_id)
        if not portfolio:
            return jsonify({'error': 'Portfolio not found'}), 404
        
        if not portfolio['positions']:
            # No positions - return simple values
            return jsonify({
                'portfolio_id': portfolio_id,
                'total_value': portfolio['available_cash'],
                'available_cash': portfolio['available_cash'],
                'total_pnl': 0.0,
                'total_pnl_percent': 0.0,
                'positions_count': 0,
                'last_updated': datetime.now().isoformat()
            })
        
        # Calculate real-time portfolio value
        total_invested = 0.0
        current_value = 0.0
        
        for symbol, position in portfolio['positions'].items():
            # Get current market price
            current_data = stock_service.get_stock_data(symbol)
            if current_data and not current_data.get('error'):
                current_price = current_data['current_price']
                position_value = position['quantity'] * current_price
                current_value += position_value
                total_invested += position['total_cost']
            else:
                # Fallback to stored price if live price unavailable
                position_value = position['quantity'] * position['avg_price']
                current_value += position_value
                total_invested += position['total_cost']
        
        total_pnl = current_value - total_invested
        total_pnl_percent = (total_pnl / total_invested * 100) if total_invested > 0 else 0.0
        total_value = current_value + portfolio['available_cash']
        
        return jsonify({
            'portfolio_id': portfolio_id,
            'total_value': total_value,
            'available_cash': portfolio['available_cash'],
            'invested_value': current_value,
            'total_cost': total_invested,
            'total_pnl': total_pnl,
            'total_pnl_percent': total_pnl_percent,
            'positions_count': len(portfolio['positions']),
            'last_updated': datetime.now().isoformat()
        })
        
    except Exception as e:
        print(f"‚ùå Error getting portfolio value: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/portfolios/<portfolio_id>/positions', methods=['GET'])
def get_portfolio_positions(portfolio_id):
    """Get portfolio positions with current prices"""
    try:
        portfolio = portfolio_manager.get_portfolio(portfolio_id)
        if not portfolio:
            return jsonify({'error': 'Portfolio not found'}), 404
        
        positions = {}
        for symbol, position in portfolio['positions'].items():
            # Get current market price
            current_data = stock_service.get_stock_data(symbol)
            current_price = position['avg_price']  # Default fallback
            
            if current_data and not current_data.get('error'):
                current_price = current_data['current_price']
            
            positions[symbol] = {
                'symbol': symbol,
                'quantity': position['quantity'],
                'avg_price': position['avg_price'],
                'total_cost': position['total_cost'],
                'current_price': current_price,
                'current_value': position['quantity'] * current_price,
                'pnl': (position['quantity'] * current_price) - position['total_cost'],
                'pnl_percent': ((current_price - position['avg_price']) / position['avg_price'] * 100) if position['avg_price'] > 0 else 0.0,
                'last_updated': position.get('last_updated', datetime.now().isoformat())
            }
        
        return jsonify({
            'portfolio_id': portfolio_id,
            'positions': positions,
            'positions_count': len(positions),
            'last_updated': datetime.now().isoformat()
        })
        
    except Exception as e:
        print(f"‚ùå Error getting portfolio positions: {e}")
        return jsonify({'error': str(e)}), 500

# Real-time Market News Integration
from textblob import TextBlob
from bs4 import BeautifulSoup
import urllib.parse
import xml.etree.ElementTree as ET
import feedparser

class MarketNewsService:
    def __init__(self):
        self.cache = {}
        self.cache_ttl = 300  # 5 minutes cache
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; TradingBot/1.0; +https://tradingplatform.com)'
        })
        
        # RSS feeds configuration - more reliable than scraping
        self.news_sources = {
            'economic_times': {
                'url': 'https://economictimes.indiatimes.com/markets/rssfeeds/1977021501.cms',
                'type': 'rss',
                'category': 'indian_markets',
                'priority': 1
            },
            'moneycontrol': {
                'url': 'https://www.moneycontrol.com/rss/marketsnews.xml',
                'type': 'rss', 
                'category': 'indian_markets',
                'priority': 1
            },
            'business_standard': {
                'url': 'https://www.business-standard.com/rss/markets-106.rss',
                'type': 'rss',
                'category': 'indian_markets', 
                'priority': 2
            },
            'reuters_markets': {
                'url': 'https://www.reuters.com/business/finance',
                'type': 'fallback',
                'category': 'global_markets',
                'priority': 3
            },
            'yahoo_finance': {
                'url': 'https://finance.yahoo.com/news/rssindex',
                'type': 'rss',
                'category': 'global_markets',
                'priority': 2
            }
        }
        
        # Fallback Indian market data
        self.fallback_indian_data = [
            {
                'title': 'NIFTY 50 Shows Strong Performance Amid Market Volatility',
                'summary': 'Indian benchmark index NIFTY 50 continues to demonstrate resilience with banking and IT sectors leading gains. Market analysts remain optimistic about sustained growth.',
                'category': 'indian_markets',
                'sentiment': 'positive'
            },
            {
                'title': 'BSE SENSEX Tracks Global Markets with Selective Buying',
                'summary': 'BSE SENSEX reflects cautious optimism as investors focus on quality stocks. Large-cap stocks in pharma and FMCG sectors attract institutional interest.',
                'category': 'indian_markets', 
                'sentiment': 'neutral'
            },
            {
                'title': 'Indian IT Sector Maintains Export Growth Momentum',
                'summary': 'Leading IT companies like TCS, Infosys, and Wipro continue to report strong quarterly results driven by digital transformation demand globally.',
                'category': 'indian_markets',
                'sentiment': 'positive'
            }
        ]
        
    def _is_cache_valid(self, cache_key):
        """Check if cached news data is still valid"""
        if cache_key not in self.cache:
            return False
        return (time.time() - self.cache[cache_key]['timestamp']) < self.cache_ttl
    
    def _get_sentiment_score(self, text):
        """Calculate basic sentiment score without TextBlob dependency"""
        try:
            text_lower = text.lower()
            
            # Positive keywords
            positive_words = ['gain', 'rise', 'up', 'growth', 'profit', 'rally', 'surge', 'bullish', 'strong', 'positive', 'advance', 'boost']
            positive_count = sum(1 for word in positive_words if word in text_lower)
            
            # Negative keywords  
            negative_words = ['fall', 'drop', 'down', 'loss', 'decline', 'crash', 'bearish', 'weak', 'negative', 'plunge', 'slide']
            negative_count = sum(1 for word in negative_words if word in text_lower)
            
            # Calculate sentiment
            if positive_count > negative_count:
                sentiment = 'positive'
                confidence = min(0.9, 0.5 + (positive_count - negative_count) * 0.1)
            elif negative_count > positive_count:
                sentiment = 'negative' 
                confidence = min(0.9, 0.5 + (negative_count - positive_count) * 0.1)
            else:
                sentiment = 'neutral'
                confidence = 0.5
                
            return sentiment, confidence
            
        except Exception as e:
            print(f"Sentiment analysis error: {e}")
            return 'neutral', 0.5
    
    def _fetch_rss_news(self, source_key, max_articles=5):
        """Fetch news from RSS feeds using feedparser"""
        try:
            source_config = self.news_sources[source_key]
            if source_config['type'] != 'rss':
                return []
                
            print(f"üîç Fetching RSS news from {source_key}...")
            
            # Parse RSS feed
            feed = feedparser.parse(source_config['url'])
            
            if not feed.entries:
                print(f"‚ö†Ô∏è No entries found in RSS feed for {source_key}")
                return []
            
            articles = []
            for i, entry in enumerate(feed.entries[:max_articles]):
                try:
                    title = entry.get('title', '').strip()
                    summary = entry.get('summary', entry.get('description', ''))
                    if summary:
                        # Clean HTML tags from summary
                        import re
                        summary = re.sub('<[^<]+?>', '', summary).strip()
                        
                    if len(summary) > 200:
                        summary = summary[:200] + "..."
                    
                    # Get publish date
                    pub_date = entry.get('published_parsed')
                    if pub_date:
                        pub_datetime = datetime(*pub_date[:6])
                        timestamp = pub_datetime.isoformat()
                    else:
                        timestamp = datetime.now().isoformat()
                    
                    # Calculate sentiment
                    sentiment_text = f"{title} {summary}"
                    sentiment, confidence = self._get_sentiment_score(sentiment_text)
                    
                    article = {
                        'id': f"{source_key}_{i}_{int(time.time())}",
                        'title': title,
                        'summary': summary or title,
                        'content': summary or title,
                        'sentiment': sentiment,
                        'sentiment_score': confidence,
                        'source': source_key.replace('_', ' ').title(),
                        'category': source_config['category'],
                        'timestamp': timestamp,
                        'url': entry.get('link', ''),
                        'relevance': min(0.9, confidence + 0.3),
                        'tags': self._extract_tags(title + " " + summary)
                    }
                    
                    articles.append(article)
                    
                except Exception as e:
                    print(f"Error processing RSS entry from {source_key}: {e}")
                    continue
            
            print(f"‚úÖ Successfully fetched {len(articles)} articles from RSS {source_key}")
            return articles
            
        except Exception as e:
            print(f"‚ùå Error fetching RSS from {source_key}: {e}")
            return []
    
    def _get_fallback_articles(self):
        """Get fallback Indian market articles when RSS fails"""
        articles = []
        for i, article_data in enumerate(self.fallback_indian_data):
            sentiment, confidence = self._get_sentiment_score(article_data['title'] + " " + article_data['summary'])
            
            article = {
                'id': f"fallback_{i}_{int(time.time())}",
                'title': article_data['title'],
                'summary': article_data['summary'],
                'content': article_data['summary'],
                'sentiment': article_data.get('sentiment', sentiment),
                'sentiment_score': confidence,
                'source': 'Market Intelligence',
                'category': article_data['category'],
                'timestamp': datetime.now().isoformat(),
                'url': '#',
                'relevance': 0.8,
                'tags': self._extract_tags(article_data['title'] + " " + article_data['summary'])
            }
            articles.append(article)
        
        return articles

    def _scrape_news_source(self, source_key, max_articles=5):
        """Legacy scraping method - now returns fallback or RSS data"""
        # Try RSS first
        if self.news_sources[source_key]['type'] == 'rss':
            return self._fetch_rss_news(source_key, max_articles)
        
        # For non-RSS sources, return empty (will trigger fallback)
        return []
    
    def _extract_tags(self, text):
        """Extract relevant tags from article text"""
        # Common market-related keywords
        keywords = [
            'nifty', 'sensex', 'bank', 'stock', 'market', 'trading', 'rally', 'fall',
            'earnings', 'revenue', 'profit', 'loss', 'ipo', 'dividend', 'merger',
            'acquisition', 'fed', 'rbi', 'interest', 'rate', 'inflation', 'gdp',
            'sector', 'technology', 'pharma', 'auto', 'finance', 'energy', 'metals'
        ]
        
        text_lower = text.lower()
        found_tags = [keyword for keyword in keywords if keyword in text_lower]
        return found_tags[:5]  # Limit to 5 tags
    
    def get_market_news(self, max_articles_per_source=3):
        """Fetch latest market news from RSS feeds with robust fallback"""
        cache_key = 'market_news_all'
        
        # Check cache first
        if self._is_cache_valid(cache_key):
            print("üì∞ Returning cached news data")
            return self.cache[cache_key]['data']
        
        print("üöÄ Fetching fresh market news from RSS feeds...")
        
        all_articles = []
        working_sources = 0
        
        # Try RSS sources first (prioritize Indian sources)
        rss_sources = [(k, v) for k, v in self.news_sources.items() if v['type'] == 'rss']
        rss_sources.sort(key=lambda x: x[1]['priority'])  # Sort by priority
        
        for source_key, source_config in rss_sources:
            try:
                articles = self._fetch_rss_news(source_key, max_articles_per_source)
                if articles:
                    all_articles.extend(articles)
                    working_sources += 1
                    print(f"‚úÖ Got {len(articles)} articles from {source_key}")
                
                # Stop after getting enough articles from primary sources
                if working_sources >= 2 and len(all_articles) >= 5:
                    break
                    
                # Small delay between sources
                time.sleep(0.3)
                
            except Exception as e:
                print(f"‚ö†Ô∏è RSS failed for {source_key}: {e}")
                continue
        
        # If RSS sources failed or gave too few articles, add fallback
        if len(all_articles) < 3:
            print("üì∞ Adding fallback Indian market articles...")
            fallback_articles = self._get_fallback_articles()
            all_articles.extend(fallback_articles)
            
        # If still no articles, create basic market status
        if not all_articles:
            all_articles = [{
                'id': f"system_status_{int(time.time())}",
                'title': "Trading Dashboard Market Intelligence Active",
                'summary': "Real-time Indian market data and analysis available. NIFTY 50, BSE SENSEX tracking, technical indicators, and portfolio management systems operational.",
                'content': "The trading platform is monitoring Indian markets including NSE and BSE indices. Key stocks like Reliance, TCS, HDFC Bank, and other blue-chip companies are being tracked with real-time price updates.",
                'sentiment': 'neutral',
                'sentiment_score': 0.5,
                'source': 'Trading Platform',
                'category': 'indian_markets',
                'timestamp': datetime.now().isoformat(),
                'url': '#',
                'relevance': 1.0,
                'tags': ['nifty', 'sensex', 'indian', 'market', 'trading']
            }]
        
        # Sort by relevance and timestamp
        all_articles.sort(key=lambda x: (x['relevance'], x['timestamp']), reverse=True)
        
        # Limit total articles
        all_articles = all_articles[:15]
        
        # Calculate overall sentiment
        sentiment_summary = self._calculate_overall_sentiment(all_articles)
        
        # Create market indicators
        market_indicators = self._generate_market_indicators(all_articles)
        
        news_data = {
            'articles': all_articles,
            'sentiment_summary': sentiment_summary,
            'market_indicators': market_indicators,
            'last_updated': datetime.now().isoformat(),
            'total_sources': len(self.news_sources),
            'articles_count': len(all_articles),
            'update_frequency': '5 minutes',
            'api_version': '2.1'
        }
        
        # Cache the results
        self.cache[cache_key] = {
            'data': news_data,
            'timestamp': time.time()
        }
        
        print(f"‚úÖ Market news compiled: {len(all_articles)} articles from {len(self.news_sources)} sources")
        return news_data
    
    def _calculate_overall_sentiment(self, articles):
        """Calculate overall sentiment from all articles"""
        if not articles:
            return {'positive': 33, 'neutral': 34, 'negative': 33, 'overall': 'neutral', 'confidence': 0.5}
        
        sentiments = [article['sentiment'] for article in articles]
        total = len(sentiments)
        
        positive_count = sum(1 for s in sentiments if s == 'positive')
        neutral_count = sum(1 for s in sentiments if s == 'neutral')
        negative_count = sum(1 for s in sentiments if s == 'negative')
        
        # Calculate confidence as average of sentiment scores
        avg_confidence = sum(article['sentiment_score'] for article in articles) / total
        
        # Determine overall sentiment
        if positive_count > negative_count and positive_count > neutral_count:
            overall = 'positive'
        elif negative_count > positive_count and negative_count > neutral_count:
            overall = 'negative'
        else:
            overall = 'neutral'
        
        return {
            'positive': round((positive_count / total) * 100, 1),
            'neutral': round((neutral_count / total) * 100, 1),
            'negative': round((negative_count / total) * 100, 1),
            'total_articles': total,
            'overall': overall,
            'confidence': round(avg_confidence, 2)
        }
    
    def _generate_market_indicators(self, articles):
        """Generate market indicators based on news sentiment"""
        sentiment_scores = [article['sentiment_score'] for article in articles]
        
        if not sentiment_scores:
            return {
                'market_mood': 'neutral',
                'volatility_index': 20.0,
                'fear_greed_index': 50.0,
                'trend_strength': 0.5
            }
        
        avg_sentiment = sum(sentiment_scores) / len(sentiment_scores)
        
        # Generate realistic indicators based on sentiment
        market_mood = 'positive' if avg_sentiment > 0.6 else 'negative' if avg_sentiment < 0.4 else 'neutral'
        
        # Volatility tends to be higher with more extreme sentiments
        volatility = 15 + (abs(avg_sentiment - 0.5) * 20)
        
        # Fear/Greed index based on sentiment (inverse for fear)
        fear_greed = 30 + (avg_sentiment * 40)
        
        # Trend strength based on sentiment consistency
        sentiment_deviation = np.std(sentiment_scores) if len(sentiment_scores) > 1 else 0
        trend_strength = max(0.3, 1 - sentiment_deviation)
        
        return {
            'market_mood': market_mood,
            'volatility_index': round(volatility, 1),
            'fear_greed_index': round(fear_greed, 1),
            'trend_strength': round(trend_strength, 2)
        }
    
    def _get_fallback_articles(self):
        """Minimal fallback when scraping fails"""
        return [
            {
                'id': f"fallback_{int(time.time())}",
                'title': "Market News Temporarily Unavailable",
                'summary': "Real-time market news is currently being updated. Please check back in a few minutes.",
                'content': "News aggregation service is temporarily unavailable.",
                'sentiment': 'neutral',
                'sentiment_score': 0.5,
                'source': 'System',
                'category': 'system',
                'timestamp': datetime.now().isoformat(),
                'url': '',
                'relevance': 0.5,
                'tags': ['system', 'update']
            }
        ]

# Initialize news service
market_news_service = MarketNewsService()

@app.route('/api/market/news', methods=['GET'])
def get_market_news():
    """
    Get real-time market news with sentiment analysis
    Returns actual news headlines from multiple financial sources
    """
    try:
        print("üì∞ Market news API called")
        
        # Get query parameters
        limit = request.args.get('limit', 15, type=int)
        category = request.args.get('category', 'all')
        
        # Fetch news data
        news_data = market_news_service.get_market_news()
        
        # Filter by category if specified
        if category != 'all':
            news_data['articles'] = [
                article for article in news_data['articles'] 
                if article['category'] == category
            ]
        
        # Limit results
        news_data['articles'] = news_data['articles'][:limit]
        news_data['filtered_count'] = len(news_data['articles'])
        
        print(f"‚úÖ Returning {len(news_data['articles'])} articles")
        
        return jsonify(news_data)
        
    except Exception as e:
        print(f"‚ùå Market news API error: {str(e)}")
        return jsonify({
            'error': 'Failed to fetch market news',
            'message': str(e),
            'timestamp': datetime.now().isoformat(),
            'articles': [],
            'sentiment_summary': {
                'positive': 33,
                'neutral': 34, 
                'negative': 33,
                'overall': 'neutral',
                'confidence': 0.5
            }
        }), 500

@app.route('/api/market/trending', methods=['GET'])
def get_trending_topics():
    """
    Get trending market topics based on news and social sentiment
    """
    try:
        print("üìà Trending topics API called")
        
        # Get fresh news data
        news_data = market_news_service.get_market_news()
        articles = news_data.get('articles', [])
        
        # Extract trending topics from news articles
        trending_topics = []
        topic_frequency = {}
        topic_sentiment = {}
        
        # Analyze articles for trending keywords
        for article in articles[:20]:  # Use recent articles
            title_lower = article.get('title', '').lower()
            tags = article.get('tags', [])
            sentiment_score = article.get('sentiment_score', 0.5)
            
            # Extract topics from tags and title
            for tag in tags:
                if len(tag) > 2:  # Skip short tags
                    topic_frequency[tag] = topic_frequency.get(tag, 0) + 1
                    topic_sentiment[tag] = topic_sentiment.get(tag, []) + [sentiment_score]
            
            # Extract common market terms from title
            market_terms = [
                'earnings', 'fed', 'rate', 'inflation', 'gdp', 'market', 
                'stock', 'rally', 'fall', 'surge', 'drop', 'ipo', 'merger',
                'dividend', 'revenue', 'profit', 'loss', 'bank', 'tech'
            ]
            
            for term in market_terms:
                if term in title_lower:
                    topic_frequency[term] = topic_frequency.get(term, 0) + 1
                    topic_sentiment[term] = topic_sentiment.get(term, []) + [sentiment_score]
        
        # Create trending topics list
        for topic, frequency in sorted(topic_frequency.items(), key=lambda x: x[1], reverse=True):
            if frequency >= 2:  # Only topics mentioned multiple times
                sentiment_scores = topic_sentiment.get(topic, [0.5])
                avg_sentiment = sum(sentiment_scores) / len(sentiment_scores)
                
                trending_topics.append({
                    'topic': topic.title(),
                    'frequency': frequency,
                    'mentions': frequency,
                    'sentiment_score': round(avg_sentiment, 2),
                    'sentiment': 'positive' if avg_sentiment > 0.6 else 'negative' if avg_sentiment < 0.4 else 'neutral',
                    'trend_strength': min(100, (frequency / max(topic_frequency.values()) * 100)) if topic_frequency else 0,
                    'category': 'market',
                    'last_updated': datetime.now().isoformat()
                })
        
        # Add some default trending topics if none found
        if not trending_topics:
            trending_topics = [
                {
                    'topic': 'Market Update',
                    'frequency': 1,
                    'mentions': 1,
                    'sentiment_score': 0.5,
                    'sentiment': 'neutral',
                    'trend_strength': 50,
                    'category': 'general',
                    'last_updated': datetime.now().isoformat()
                },
                {
                    'topic': 'Economic Indicators',
                    'frequency': 1,
                    'mentions': 1,
                    'sentiment_score': 0.5,
                    'sentiment': 'neutral', 
                    'trend_strength': 40,
                    'category': 'economic',
                    'last_updated': datetime.now().isoformat()
                }
            ]
        
        # Limit to top 10 trending topics
        trending_topics = trending_topics[:10]
        
        response_data = {
            'trending_topics': trending_topics,
            'total_topics': len(trending_topics),
            'data_source': 'news_analysis',
            'analysis_period': '24h',
            'last_updated': datetime.now().isoformat(),
            'api_version': '1.0'
        }
        
        print(f"‚úÖ Returning {len(trending_topics)} trending topics")
        return jsonify(response_data)
        
    except Exception as e:
        print(f"‚ùå Trending topics API error: {str(e)}")
        return jsonify({
            'error': 'Failed to fetch trending topics',
            'message': str(e),
            'trending_topics': [],
            'total_topics': 0,
            'last_updated': datetime.now().isoformat()
        }), 500

@app.route('/api/market/indian-news', methods=['GET'])
def get_indian_market_news():
    """
    Get India-focused market news with sentiment analysis
    """
    try:
        print("üáÆüá≥ Indian market news API called")
        
        # Get general news first
        news_data = market_news_service.get_market_news()
        all_articles = news_data.get('articles', [])
        
        # Filter for India-related content
        indian_keywords = [
            'india', 'indian', 'nifty', 'sensex', 'bse', 'nse', 'mumbai', 'rupee', 'rbi', 
            'reliance', 'tcs', 'infosys', 'hdfc', 'icici', 'sbi', 'airtel', 'itc',
            'delhi', 'bangalore', 'chennai', 'hyderabad', 'pune', 'gujarat', 'maharashtra',
            'modi', 'sebi', 'gst', 'budget', 'economic survey', 'demonetization',
            'make in india', 'digital india', 'startup india', 'ayushman bharat'
        ]
        
        indian_articles = []
        for article in all_articles:
            title_lower = article.get('title', '').lower()
            summary_lower = article.get('summary', '').lower()
            content_text = f"{title_lower} {summary_lower}"
            
            # Check if article mentions India-related keywords
            indian_relevance_score = 0
            for keyword in indian_keywords:
                if keyword in content_text:
                    indian_relevance_score += 1
            
            if indian_relevance_score > 0:
                # Add India relevance score
                article_copy = article.copy()
                article_copy['india_relevance'] = indian_relevance_score / len(indian_keywords)
                article_copy['region'] = 'India'
                
                # Enhance tags with India-specific tags
                existing_tags = article_copy.get('tags', [])
                if 'nifty' in content_text or 'sensex' in content_text:
                    existing_tags.append('Indian Markets')
                if 'rupee' in content_text:
                    existing_tags.append('INR')
                if any(company in content_text for company in ['reliance', 'tcs', 'infosys']):
                    existing_tags.append('Indian Stocks')
                if 'rbi' in content_text:
                    existing_tags.append('RBI Policy')
                
                article_copy['tags'] = list(set(existing_tags))
                indian_articles.append(article_copy)
        
        # Sort by India relevance and recency
        indian_articles.sort(key=lambda x: (x.get('india_relevance', 0), x.get('timestamp', '')), reverse=True)
        
        # Get limit from query parameters
        limit = request.args.get('limit', 10, type=int)
        indian_articles = indian_articles[:limit]
        
        # Calculate sentiment for India-focused articles
        sentiment_scores = [article.get('sentiment_score', 0.5) for article in indian_articles]
        avg_sentiment = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0.5
        
        positive_count = sum(1 for score in sentiment_scores if score > 0.6)
        negative_count = sum(1 for score in sentiment_scores if score < 0.4)
        neutral_count = len(sentiment_scores) - positive_count - negative_count
        
        total_articles = len(sentiment_scores)
        sentiment_summary = {
            'positive': round((positive_count / total_articles * 100), 1) if total_articles > 0 else 33,
            'neutral': round((neutral_count / total_articles * 100), 1) if total_articles > 0 else 34,
            'negative': round((negative_count / total_articles * 100), 1) if total_articles > 0 else 33,
            'overall': 'positive' if avg_sentiment > 0.6 else 'negative' if avg_sentiment < 0.4 else 'neutral',
            'confidence': round(abs(avg_sentiment - 0.5) * 2, 2),
            'avg_score': round(avg_sentiment, 3)
        }
        
        # Add some synthetic Indian market articles if none found
        if not indian_articles:
            current_time = datetime.now()
            indian_articles = [
                {
                    'title': 'Nifty 50 Shows Strong Performance Amid Global Volatility',
                    'summary': 'Indian equity markets demonstrate resilience with Nifty 50 maintaining upward momentum despite global economic headwinds.',
                    'url': '#',
                    'source': 'Market Intelligence',
                    'timestamp': (current_time - timedelta(hours=1)).isoformat(),
                    'category': 'market',
                    'sentiment_score': 0.7,
                    'sentiment': 'positive',
                    'tags': ['Nifty 50', 'Indian Markets', 'Equity'],
                    'region': 'India',
                    'india_relevance': 0.95,
                    'relevance': 0.9
                },
                {
                    'title': 'RBI Monetary Policy Decision Awaited by Market Participants',
                    'summary': 'Market participants closely watch RBI policy stance as inflation data and economic indicators suggest potential rate adjustments.',
                    'url': '#',
                    'source': 'Economic Times',
                    'timestamp': (current_time - timedelta(hours=3)).isoformat(),
                    'category': 'policy',
                    'sentiment_score': 0.5,
                    'sentiment': 'neutral',
                    'tags': ['RBI Policy', 'Monetary Policy', 'Interest Rates'],
                    'region': 'India',
                    'india_relevance': 0.9,
                    'relevance': 0.85
                }
            ]
            
            # Recalculate sentiment for synthetic articles
            sentiment_summary = {
                'positive': 50,
                'neutral': 50,
                'negative': 0,
                'overall': 'neutral',
                'confidence': 0.6,
                'avg_score': 0.6
            }
        
        response_data = {
            'articles': indian_articles,
            'total_articles': len(indian_articles),
            'region_focus': 'India',
            'sentiment_summary': sentiment_summary,
            'market_focus': {
                'primary_indices': ['Nifty 50', 'Sensex', 'Bank Nifty'],
                'currency': 'INR',
                'key_sectors': ['IT', 'Banking', 'Pharmaceuticals', 'FMCG', 'Metals']
            },
            'data_source': 'indian_market_filter',
            'last_updated': datetime.now().isoformat(),
            'api_version': '1.0'
        }
        
        print(f"‚úÖ Returning {len(indian_articles)} Indian market articles")
        return jsonify(response_data)
        
    except Exception as e:
        print(f"‚ùå Indian market news API error: {str(e)}")
        return jsonify({
            'error': 'Failed to fetch Indian market news',
            'message': str(e),
            'articles': [],
            'total_articles': 0,
            'region_focus': 'India',
            'sentiment_summary': {
                'positive': 33,
                'neutral': 34,
                'negative': 33,
                'overall': 'neutral',
                'confidence': 0.5,
                'avg_score': 0.5
            },
            'last_updated': datetime.now().isoformat()
        }), 500

# Background cleanup task
def cleanup_expired_cache():
    """Background task to cleanup expired cache"""
    while True:
        try:
            stock_service.cleanup_expired_cache()
            time.sleep(60)  # Run every minute
        except Exception as e:
            print(f"Cache cleanup error: {e}")

# Price Alerts System
class PriceAlertsManager:
    def __init__(self):
        self.alerts = []
        self.alert_counter = 1000
        self.init_alerts_database()
    
    def init_alerts_database(self):
        """Initialize alerts database table"""
        try:
            # Use the same database as the portfolio system
            db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data', 'trading_platform.db')
            os.makedirs(os.path.dirname(db_path), exist_ok=True)
            
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS price_alerts (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    symbol TEXT NOT NULL,
                    alert_type TEXT NOT NULL,
                    target_price REAL NOT NULL,
                    current_price REAL,
                    condition_type TEXT NOT NULL,
                    message TEXT,
                    is_active BOOLEAN DEFAULT 1,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    triggered_at TIMESTAMP NULL,
                    notification_sent BOOLEAN DEFAULT 0
                )
            ''')
            
            conn.commit()
            conn.close()
            print("‚úÖ Price alerts database initialized")
            
            # Load existing alerts
            self.load_alerts_from_db()
            
        except Exception as e:
            print(f"‚ùå Price alerts database init error: {e}")
    
    def load_alerts_from_db(self):
        """Load alerts from database"""
        try:
            db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data', 'trading_platform.db')
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT id, symbol, alert_type, target_price, current_price, 
                       condition_type, message, is_active, created_at, 
                       triggered_at, notification_sent
                FROM price_alerts 
                WHERE is_active = 1
                ORDER BY created_at DESC
            ''')
            
            alerts_data = cursor.fetchall()
            self.alerts = []
            
            for alert_row in alerts_data:
                alert = {
                    'id': alert_row[0],
                    'symbol': alert_row[1],
                    'alert_type': alert_row[2],
                    'target_price': alert_row[3],
                    'current_price': alert_row[4],
                    'condition': alert_row[5],
                    'message': alert_row[6],
                    'is_active': bool(alert_row[7]),
                    'created_at': alert_row[8],
                    'triggered_at': alert_row[9],
                    'notification_sent': bool(alert_row[10]),
                    'status': 'active' if alert_row[7] else 'inactive'
                }
                self.alerts.append(alert)
            
            conn.close()
            print(f"‚úÖ Loaded {len(self.alerts)} price alerts from database")
            
        except Exception as e:
            print(f"‚ùå Error loading alerts: {e}")
            self.alerts = []
    
    def create_alert(self, symbol, alert_type, target_price, condition, message=""):
        """Create a new price alert"""
        try:
            # Validate inputs
            symbol = symbol.upper().strip()
            if not symbol or not target_price:
                return {'error': 'Symbol and target price are required'}
            
            # Get current price for reference
            try:
                current_stock_data = stock_service.get_stock_data(symbol, period="1d", interval="1m")
                current_price = current_stock_data.get('current_price', 0) if current_stock_data else 0
            except:
                current_price = 0
            
            # Create alert object
            alert = {
                'id': self.alert_counter,
                'symbol': symbol,
                'alert_type': alert_type,
                'target_price': float(target_price),
                'current_price': current_price,
                'condition': condition,
                'message': message or f"{alert_type} alert for {symbol} at {target_price}",
                'is_active': True,
                'created_at': datetime.now().isoformat(),
                'triggered_at': None,
                'notification_sent': False,
                'status': 'active'
            }
            
            # Save to database
            db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data', 'trading_platform.db')
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO price_alerts 
                (symbol, alert_type, target_price, current_price, condition_type, message, is_active)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (symbol, alert_type, target_price, current_price, condition, alert['message'], 1))
            
            alert['id'] = cursor.lastrowid
            conn.commit()
            conn.close()
            
            # Add to memory
            self.alerts.append(alert)
            self.alert_counter += 1
            
            print(f"‚úÖ Created price alert: {symbol} {condition} {target_price}")
            return {'success': True, 'alert': alert}
            
        except Exception as e:
            print(f"‚ùå Error creating alert: {e}")
            return {'error': f'Failed to create alert: {str(e)}'}
    
    def get_alerts(self, symbol=None, active_only=True):
        """Get alerts, optionally filtered by symbol"""
        try:
            alerts = self.alerts.copy()
            
            if active_only:
                alerts = [a for a in alerts if a.get('is_active', False)]
            
            if symbol:
                symbol = symbol.upper()
                alerts = [a for a in alerts if a.get('symbol', '').upper() == symbol]
            
            # Sort by creation date (newest first)
            alerts.sort(key=lambda x: x.get('created_at', ''), reverse=True)
            
            return {'success': True, 'alerts': alerts, 'total': len(alerts)}
            
        except Exception as e:
            print(f"‚ùå Error getting alerts: {e}")
            return {'error': f'Failed to get alerts: {str(e)}', 'alerts': [], 'total': 0}
    
    def delete_alert(self, alert_id):
        """Delete/deactivate an alert"""
        try:
            alert_id = int(alert_id)
            
            # Find alert in memory
            alert_found = False
            for alert in self.alerts:
                if alert.get('id') == alert_id:
                    alert['is_active'] = False
                    alert['status'] = 'deleted'
                    alert_found = True
                    break
            
            if not alert_found:
                return {'error': 'Alert not found'}
            
            # Update database
            db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data', 'trading_platform.db')
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            cursor.execute('UPDATE price_alerts SET is_active = 0 WHERE id = ?', (alert_id,))
            conn.commit()
            conn.close()
            
            print(f"‚úÖ Deleted price alert ID: {alert_id}")
            return {'success': True, 'message': 'Alert deleted successfully'}
            
        except Exception as e:
            print(f"‚ùå Error deleting alert: {e}")
            return {'error': f'Failed to delete alert: {str(e)}'}
    
    def check_alerts(self):
        """Check all active alerts against current prices"""
        triggered_alerts = []
        
        try:
            active_alerts = [a for a in self.alerts if a.get('is_active', False)]
            
            for alert in active_alerts:
                try:
                    symbol = alert.get('symbol')
                    target_price = alert.get('target_price')
                    condition = alert.get('condition')
                    
                    # Get current price
                    stock_data = stock_service.get_stock_data(symbol, period="1d", interval="1m")
                    current_price = stock_data.get('current_price', 0) if stock_data else 0
                    
                    if current_price <= 0:
                        continue
                    
                    # Update current price in alert
                    alert['current_price'] = current_price
                    
                    # Check if alert condition is met
                    triggered = False
                    if condition == 'above' and current_price >= target_price:
                        triggered = True
                    elif condition == 'below' and current_price <= target_price:
                        triggered = True
                    elif condition == 'equals' and abs(current_price - target_price) <= (target_price * 0.01):  # 1% tolerance
                        triggered = True
                    
                    if triggered and not alert.get('notification_sent', False):
                        # Mark as triggered
                        alert['triggered_at'] = datetime.now().isoformat()
                        alert['notification_sent'] = True
                        alert['status'] = 'triggered'
                        
                        # Update database
                        db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data', 'trading_platform.db')
                        conn = sqlite3.connect(db_path)
                        cursor = conn.cursor()
                        cursor.execute('''
                            UPDATE price_alerts 
                            SET current_price = ?, triggered_at = ?, notification_sent = 1
                            WHERE id = ?
                        ''', (current_price, alert['triggered_at'], alert['id']))
                        conn.commit()
                        conn.close()
                        
                        triggered_alerts.append(alert)
                        print(f"üö® Alert triggered: {symbol} {condition} {target_price} (current: {current_price})")
                
                except Exception as alert_error:
                    print(f"‚ùå Error checking alert {alert.get('id', 'unknown')}: {alert_error}")
                    continue
        
        except Exception as e:
            print(f"‚ùå Error in check_alerts: {e}")
        
        return triggered_alerts

# Initialize price alerts manager
price_alerts_manager = PriceAlertsManager()

# Price Alerts API Endpoints
@app.route('/api/alerts', methods=['GET'])
def get_price_alerts():
    """Get all price alerts"""
    try:
        symbol = request.args.get('symbol')
        active_only = request.args.get('active_only', 'true').lower() == 'true'
        
        result = price_alerts_manager.get_alerts(symbol=symbol, active_only=active_only)
        return jsonify(result)
        
    except Exception as e:
        return jsonify({'error': f'Failed to get alerts: {str(e)}', 'alerts': [], 'total': 0}), 500

@app.route('/api/alerts', methods=['POST'])
def create_price_alert():
    """Create a new price alert"""
    try:
        data = request.get_json() or {}
        
        symbol = data.get('symbol', '').strip().upper()
        alert_type = data.get('type', 'price').lower()
        target_price = data.get('target_price')
        condition = data.get('condition', 'above').lower()
        message = data.get('message', '')
        
        if not symbol or not target_price:
            return jsonify({'error': 'Symbol and target_price are required'}), 400
        
        try:
            target_price = float(target_price)
        except (ValueError, TypeError):
            return jsonify({'error': 'Invalid target_price format'}), 400
        
        if condition not in ['above', 'below', 'equals']:
            return jsonify({'error': 'Condition must be: above, below, or equals'}), 400
        
        result = price_alerts_manager.create_alert(symbol, alert_type, target_price, condition, message)
        
        if 'error' in result:
            return jsonify(result), 400
        
        return jsonify(result), 201
        
    except Exception as e:
        return jsonify({'error': f'Failed to create alert: {str(e)}'}), 500

@app.route('/api/alerts/<alert_id>', methods=['DELETE'])
def delete_price_alert(alert_id):
    """Delete a price alert"""
    try:
        result = price_alerts_manager.delete_alert(alert_id)
        
        if 'error' in result:
            return jsonify(result), 404
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({'error': f'Failed to delete alert: {str(e)}'}), 500

@app.route('/api/alerts/check', methods=['POST'])
def check_price_alerts():
    """Manually trigger alert checking"""
    try:
        triggered = price_alerts_manager.check_alerts()
        
        return jsonify({
            'success': True,
            'triggered_alerts': triggered,
            'total_triggered': len(triggered),
            'message': f'Checked alerts, {len(triggered)} triggered'
        })
        
    except Exception as e:
        return jsonify({'error': f'Failed to check alerts: {str(e)}'}), 500

# Background alert checking
def background_alert_checker():
    """Background task to check price alerts periodically"""
    while True:
        try:
            triggered = price_alerts_manager.check_alerts()
            if triggered:
                print(f"üö® {len(triggered)} alerts triggered in background check")
            time.sleep(30)  # Check every 30 seconds
        except Exception as e:
            print(f"Background alert checker error: {e}")
            time.sleep(60)  # Wait longer if error

# Start background processes
cleanup_thread = threading.Thread(target=cleanup_expired_cache, daemon=True)
cleanup_thread.start()

alerts_thread = threading.Thread(target=background_alert_checker, daemon=True)
alerts_thread.start()

if __name__ == '__main__':
    import os
    
    print("üöÄ Starting Clean Trading Platform Backend...")
    print("‚úÖ Enhanced caching enabled")
    print("‚úÖ Rate limiting active") 
    print("‚úÖ Background cleanup running")
    print("üîó Server starting on http://0.0.0.0:5000")
    
    # Check if running in production (Render sets PORT env var)
    port = int(os.environ.get('PORT', 5000))
    debug_mode = os.environ.get('FLASK_ENV') != 'production'
    
    if os.environ.get('FLASK_ENV') == 'production':
        print("üåê Running in PRODUCTION mode")
        # In production, let Gunicorn handle the WSGI server
        socketio.run(app, host='0.0.0.0', port=port, debug=False, allow_unsafe_werkzeug=True)
    else:
        print("üîß Running in DEVELOPMENT mode")
        socketio.run(app, host='0.0.0.0', port=port, debug=True)
